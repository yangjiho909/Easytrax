#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import warnings
import pickle
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from soynlp.tokenizer import RegexTokenizer
warnings.filterwarnings("ignore")

def load_model():
    try:
        with open("model/vectorizer.pkl", "rb") as f:
            vectorizer = pickle.load(f)
        with open("model/indexed_matrix.pkl", "rb") as f:
            tfidf_matrix = pickle.load(f)
        with open("model/raw_data.pkl", "rb") as f:
            raw_data = pickle.load(f)
        return vectorizer, tfidf_matrix, raw_data
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return None, None, None

def analyze_input(user_input, vectorizer, tfidf_matrix, raw_data, top_k=5):
    try:
        input_vec = vectorizer.transform([user_input])
        similarities = cosine_similarity(input_vec, tfidf_matrix).flatten()
        top_indices = similarities.argsort()[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            sim = similarities[idx]
            row = raw_data.iloc[idx]
            result = {
                "í’ˆëª©": row.get("í’ˆëª©", "ì •ë³´ ì—†ìŒ"),
                "ì›ì‚°ì§€": row.get("ì›ì‚°ì§€", "ì •ë³´ ì—†ìŒ"),
                "ìˆ˜ì…êµ­": row.get("ìˆ˜ì…êµ­", "ì •ë³´ ì—†ìŒ"),
                "ì¡°ì¹˜ì‚¬í•­": row.get("ì¡°ì¹˜ì‚¬í•­", "ì •ë³´ ì—†ìŒ"),
                "ë¬¸ì œì‚¬ìœ ": row.get("ë¬¸ì œì‚¬ìœ ", "ì •ë³´ ì—†ìŒ"),
                "ìœ ì‚¬ë„": round(float(sim), 3)
            }
            results.append(result)
        
        return results
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return []

def main():
    print("âœ… KATI í†µê´€ ì‹¤íŒ¨ ì‚¬ë¡€ ê²€ìƒ‰ ì‹œìŠ¤í…œ")
    print("=" * 50)
    
    # ëª¨ë¸ ë¡œë”©
    vectorizer, tfidf_matrix, raw_data = load_model()
    if vectorizer is None:
        print("âŒ ëª¨ë¸ì„ ë¡œë”©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (ë°ì´í„°: {len(raw_data)}ê°œ)")
    
    # ì‚¬ìš©ì ì…ë ¥
    print("\nğŸ“ ìˆ˜ì¶œí•˜ê³ ì í•˜ëŠ” ì œí’ˆ ì •ë³´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    user_input = input("ì œí’ˆ ì„¤ëª…: ")
    
    if not user_input.strip():
        print("âŒ ì œí’ˆ ì„¤ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return
    
    # í† í¬ë‚˜ì´ì§•
    tokenizer = RegexTokenizer()
    user_input = " ".join(tokenizer.tokenize(user_input, flatten=True))
    
    # ë¶„ì„
    results = analyze_input(user_input, vectorizer, tfidf_matrix, raw_data)
    
    if not results:
        print("âŒ ìœ ì‚¬í•œ ì‚¬ë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“Œ ìœ ì‚¬ í†µê´€ ì‹¤íŒ¨ ì‚¬ë¡€ ({len(results)}ê°œ):")
    print("=" * 50)
    
    for i, result in enumerate(results, 1):
        print(f"\nğŸ§¾ [ì‚¬ë¡€ {i}] (ìœ ì‚¬ë„: {result['ìœ ì‚¬ë„']})")
        print(f"í’ˆëª©     : {result['í’ˆëª©']}")
        print(f"ì›ì‚°ì§€   : {result['ì›ì‚°ì§€']}")
        print(f"ìˆ˜ì…êµ­   : {result['ìˆ˜ì…êµ­']}")
        print(f"ì¡°ì¹˜ì‚¬í•­ : {result['ì¡°ì¹˜ì‚¬í•­']}")
        print(f"ë¬¸ì œì‚¬ìœ  : {result['ë¬¸ì œì‚¬ìœ ']}")
    
    print("\nğŸ” ìœ„ ì‚¬ë¡€ë“¤ì„ ì°¸ê³ í•˜ì—¬ í†µê´€ ë¦¬ìŠ¤í¬ë¥¼ ìµœì†Œí™”í•˜ì„¸ìš”.")

if __name__ == "__main__":
    main() 