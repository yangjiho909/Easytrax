#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ìƒˆë¡œìš´ ë°ì´í„°ì…‹ í†µí•© (ì¤‘êµ­, ë¯¸êµ­ ìœ„ì£¼)
"""

import pandas as pd
import pickle
import os
import warnings
from glob import glob
from soynlp.tokenizer import RegexTokenizer
from sklearn.feature_extraction.text import TfidfVectorizer
warnings.filterwarnings("ignore", category=UserWarning, module="openpyxl")
warnings.filterwarnings("ignore", category=FutureWarning, module="soynlp")

def integrate_new_data():
    """ìƒˆë¡œìš´ ë°ì´í„°ì…‹ í†µí•©"""
    
    print("ğŸš€ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ í†µí•© ì‹œì‘")
    print("=" * 60)
    
    # 1. ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    print("\nğŸ“ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì¤‘...")
    try:
        with open('model/raw_data.pkl', 'rb') as f:
            existing_df = pickle.load(f)
        print(f"âœ… ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(existing_df):,}ê°œ")
    except Exception as e:
        print(f"âŒ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # 2. ìƒˆë¡œìš´ ë°ì´í„°ì…‹ íŒŒì¼ë“¤
    new_files = [
        "ë¯¸êµ­ìœ¼ë¡œ ìˆ˜ì¶œ 2.xlsx",
        "ë¯¸êµ­ìœ¼ë¡œ ìˆ˜ì¶œ.xlsx", 
        "ì¤‘êµ­ ìœ¼ë¡œ ìˆ˜ì¶œ.xlsx"
    ]
    
    all_new_data = []
    
    for file in new_files:
        file_path = os.path.join("data", file)
        
        if os.path.exists(file_path):
            try:
                print(f"\nğŸ“ ì²˜ë¦¬ ì¤‘: {file}")
                
                # ë°ì´í„° ë¡œë“œ
                df = pd.read_excel(file_path)
                print(f"  ë ˆì½”ë“œ ìˆ˜: {len(df):,}ê°œ")
                
                # ë¬¸ì œì‚¬ìœ  ë³‘í•© ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹ê³¼ ë™ì¼)
                sa_yu_cols = [col for col in df.columns if "ë¬¸ì œì‚¬ìœ " in col]
                df["ë¬¸ì œì‚¬ìœ "] = df[sa_yu_cols].fillna("").astype(str).agg(" ".join, axis=1)
                
                # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
                required_cols = ["í’ˆëª©", "ì›ì‚°ì§€", "ìˆ˜ì…êµ­", "ì¡°ì¹˜ì‚¬í•­", "ë¬¸ì œì‚¬ìœ ", "HS CODE"]
                df = df[required_cols].dropna()
                
                # ì¶œì²˜ íŒŒì¼ ì •ë³´ ì¶”ê°€
                df["ì¶œì²˜íŒŒì¼"] = file
                
                all_new_data.append(df)
                print(f"  âœ… ì²˜ë¦¬ ì™„ë£Œ: {len(df):,}ê°œ")
                
            except Exception as e:
                print(f"  âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        else:
            print(f"âŒ íŒŒì¼ ì—†ìŒ: {file}")
    
    # 3. ìƒˆë¡œìš´ ë°ì´í„° ë³‘í•©
    if all_new_data:
        print(f"\nğŸ”— ìƒˆë¡œìš´ ë°ì´í„° ë³‘í•© ì¤‘...")
        new_df = pd.concat(all_new_data, ignore_index=True)
        print(f"âœ… ìƒˆë¡œìš´ ë°ì´í„° ë³‘í•© ì™„ë£Œ: {len(new_df):,}ê°œ")
        
        # 4. ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©
        print(f"\nğŸ”— ì „ì²´ ë°ì´í„° ë³‘í•© ì¤‘...")
        combined_df = pd.concat([existing_df, new_df], ignore_index=True)
        print(f"âœ… ì „ì²´ ë°ì´í„° ë³‘í•© ì™„ë£Œ: {len(combined_df):,}ê°œ")
        
        # 5. ì¤‘ë³µ ì œê±° (í•„ìš”í•œ ê²½ìš°)
        print(f"\nğŸ§¹ ì¤‘ë³µ ë°ì´í„° ì œê±° ì¤‘...")
        initial_count = len(combined_df)
        combined_df = combined_df.drop_duplicates(subset=['í’ˆëª©', 'ì›ì‚°ì§€', 'ìˆ˜ì…êµ­', 'ë¬¸ì œì‚¬ìœ '], keep='first')
        final_count = len(combined_df)
        removed_count = initial_count - final_count
        print(f"âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: {removed_count:,}ê°œ ì œê±°ë¨")
        
        # 6. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        print(f"\nğŸ“ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì¤‘...")
        tokenizer = RegexTokenizer()
        
        # í…ìŠ¤íŠ¸ ê²°í•©: í’ˆëª© + ì›ì‚°ì§€ + ìˆ˜ì…êµ­ + ë¬¸ì œì‚¬ìœ 
        combined_df["í…ìŠ¤íŠ¸"] = (
            combined_df["í’ˆëª©"].astype(str) + " " +
            combined_df["ì›ì‚°ì§€"].astype(str) + " " +
            combined_df["ìˆ˜ì…êµ­"].astype(str) + " " +
            combined_df["ë¬¸ì œì‚¬ìœ "].astype(str) + " " +
            combined_df["HS CODE"].astype(str)
        )
        
        # í† í°í™” ì ìš©
        combined_df["í…ìŠ¤íŠ¸"] = combined_df["í…ìŠ¤íŠ¸"].apply(lambda x: " ".join(tokenizer.tokenize(x, flatten=True)))
        
        # 7. TF-IDF ëª¨ë¸ ì¬í•™ìŠµ
        print(f"\nğŸ¤– TF-IDF ëª¨ë¸ ì¬í•™ìŠµ ì¤‘...")
        vectorizer = TfidfVectorizer()
        X = vectorizer.fit_transform(combined_df["í…ìŠ¤íŠ¸"])
        
        # 8. ëª¨ë¸ ì €ì¥
        print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
        os.makedirs("model", exist_ok=True)
        
        with open("model/vectorizer.pkl", "wb") as f:
            pickle.dump(vectorizer, f)
        with open("model/indexed_matrix.pkl", "wb") as f:
            pickle.dump(X, f)
        with open("model/raw_data.pkl", "wb") as f:
            pickle.dump(combined_df, f)
        
        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
        
        # 9. í†µí•© ê²°ê³¼ ë¶„ì„
        print(f"\nğŸ“Š í†µí•© ê²°ê³¼ ë¶„ì„")
        print("=" * 40)
        
        # êµ­ê°€ë³„ ë°ì´í„° ë¶„í¬
        country_counts = combined_df['ìˆ˜ì…êµ­'].value_counts()
        print(f"êµ­ê°€ë³„ ë°ì´í„° ë¶„í¬:")
        for country, count in country_counts.head(10).items():
            print(f"  {country}: {count:,}ê°œ")
        
        # ì¤‘êµ­, ë¯¸êµ­ ë°ì´í„° í™•ì¸
        china_data = combined_df[combined_df['ìˆ˜ì…êµ­'] == 'ì¤‘êµ­']
        us_data = combined_df[combined_df['ìˆ˜ì…êµ­'] == 'ë¯¸êµ­']
        
        print(f"\nğŸ‡¨ğŸ‡³ğŸ‡ºğŸ‡¸ ì¤‘êµ­, ë¯¸êµ­ ë°ì´í„°:")
        print(f"  ì¤‘êµ­: {len(china_data):,}ê°œ (ê¸°ì¡´: 27,249ê°œ â†’ +{len(china_data)-27249:,}ê°œ)")
        print(f"  ë¯¸êµ­: {len(us_data):,}ê°œ (ê¸°ì¡´: 73,870ê°œ â†’ +{len(us_data)-73870:,}ê°œ)")
        print(f"  ì¤‘êµ­+ë¯¸êµ­: {len(china_data) + len(us_data):,}ê°œ")
        
        # ë°ì´í„° í’ˆì§ˆ í™•ì¸
        print(f"\nğŸ“ˆ ë°ì´í„° í’ˆì§ˆ:")
        print(f"  ì „ì²´ ë°ì´í„°: {len(combined_df):,}ê°œ")
        print(f"  ê³ ìœ  í’ˆëª©: {combined_df['í’ˆëª©'].nunique():,}ê°œ")
        print(f"  ê³ ìœ  êµ­ê°€: {combined_df['ìˆ˜ì…êµ­'].nunique()}ê°œ")
        
        # ë¬¸ì œì‚¬ìœ  ê¸¸ì´ ë¶„ì„
        reason_lengths = combined_df['ë¬¸ì œì‚¬ìœ '].str.len()
        print(f"  ë¬¸ì œì‚¬ìœ  í‰ê·  ê¸¸ì´: {reason_lengths.mean():.1f}ì")
        print(f"  ë¹ˆ ë¬¸ì œì‚¬ìœ : {(combined_df['ë¬¸ì œì‚¬ìœ '].str.strip() == '').sum():,}ê°œ")
        
        return combined_df
        
    else:
        print("âŒ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

def test_integrated_system():
    """í†µí•©ëœ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    
    print(f"\nğŸ§ª í†µí•©ëœ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 40)
    
    try:
        # ìƒˆë¡œìš´ ëª¨ë¸ ë¡œë“œ
        with open('model/raw_data.pkl', 'rb') as f:
            df = pickle.load(f)
        with open('model/vectorizer.pkl', 'rb') as f:
            vectorizer = pickle.load(f)
        with open('model/indexed_matrix.pkl', 'rb') as f:
            X = pickle.load(f)
        
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        print(f"  ë°ì´í„°: {len(df):,}ê°œ")
        print(f"  ë²¡í„° í¬ê¸°: {X.shape}")
        
        # ê°„ë‹¨í•œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        test_queries = ["ì¤‘êµ­ ë¼ë©´", "ë¯¸êµ­ ê³¼ì", "ì¤‘êµ­ ì±„ì†Œ", "ë¯¸êµ­ ìˆ˜ì‚°ë¬¼"]
        
        for query in test_queries:
            # ì¿¼ë¦¬ ì „ì²˜ë¦¬
            query_vector = vectorizer.transform([query])
            
            # ìœ ì‚¬ë„ ê³„ì‚°
            from sklearn.metrics.pairwise import cosine_similarity
            similarities = cosine_similarity(query_vector, X).flatten()
            
            # ìƒìœ„ ê²°ê³¼ ì°¾ê¸°
            top_indices = similarities.argsort()[-5:][::-1]
            
            print(f"\nğŸ” '{query}' ê²€ìƒ‰ ê²°ê³¼:")
            for i, idx in enumerate(top_indices, 1):
                if similarities[idx] > 0.1:  # ì„ê³„ê°’
                    result = df.iloc[idx]
                    print(f"  {i}. {result['í’ˆëª©']} ({result['ìˆ˜ì…êµ­']}) - ìœ ì‚¬ë„: {similarities[idx]:.3f}")
        
        print(f"\nâœ… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    # 1. ìƒˆë¡œìš´ ë°ì´í„° í†µí•©
    integrated_df = integrate_new_data()
    
    if integrated_df is not None:
        # 2. í†µí•©ëœ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
        test_integrated_system()
        
        print(f"\nğŸ‰ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ í†µí•© ì™„ë£Œ!")
        print(f"ğŸ“Š ìµœì¢… ë°ì´í„°: {len(integrated_df):,}ê°œ")
    else:
        print(f"\nâŒ ë°ì´í„° í†µí•© ì‹¤íŒ¨") 