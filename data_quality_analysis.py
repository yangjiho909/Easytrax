#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import pickle
import pandas as pd
import numpy as np
from collections import Counter

def analyze_data_quality():
    """ë°ì´í„° í’ˆì§ˆ ë° ëª©ì  ì í•©ì„± ë¶„ì„"""
    print("ğŸ” KATI ë°ì´í„° í’ˆì§ˆ ë° ëª©ì  ì í•©ì„± ë¶„ì„")
    print("=" * 70)
    
    try:
        # ë°ì´í„° ë¡œë”©
        with open("model/raw_data.pkl", "rb") as f:
            raw_data = pickle.load(f)
        
        print(f"ğŸ“Š ì´ ë°ì´í„° ìˆ˜: {len(raw_data):,}ê±´")
        print("=" * 70)
        
        # 1. ê¸°ë³¸ ë°ì´í„° êµ¬ì¡° ë¶„ì„
        print("ğŸ“‹ 1. ë°ì´í„° êµ¬ì¡° ë¶„ì„")
        print("-" * 40)
        print(f"ì»¬ëŸ¼ ìˆ˜: {len(raw_data.columns)}")
        print(f"ì»¬ëŸ¼ëª…: {list(raw_data.columns)}")
        
        # 2. ê²°ì¸¡ê°’ ë¶„ì„
        print(f"\nğŸ“‹ 2. ê²°ì¸¡ê°’ ë¶„ì„")
        print("-" * 40)
        for col in raw_data.columns:
            missing = raw_data[col].isnull().sum()
            missing_pct = (missing / len(raw_data)) * 100
            print(f"{col:<15}: {missing:>6}ê±´ ({missing_pct:>5.1f}%)")
        
        # 3. í•µì‹¬ ì»¬ëŸ¼ë³„ ìƒì„¸ ë¶„ì„
        print(f"\nğŸ“‹ 3. í•µì‹¬ ì»¬ëŸ¼ë³„ ìƒì„¸ ë¶„ì„")
        print("-" * 40)
        
        # ìˆ˜ì…êµ­ ë¶„ì„
        if "ìˆ˜ì…êµ­" in raw_data.columns:
            countries = raw_data["ìˆ˜ì…êµ­"].dropna()
            print(f"ğŸŒ ìˆ˜ì…êµ­:")
            print(f"   - ê³ ìœ ê°’: {countries.nunique()}ê°œ")
            print(f"   - ìƒìœ„ 5ê°œ: {list(countries.value_counts().head().index)}")
            print(f"   - í•œêµ­ ê´€ë ¨: {len(countries[countries.str.contains('í•œêµ­|ëŒ€í•œë¯¼êµ­', na=False)])}ê±´")
        
        # ì›ì‚°ì§€ ë¶„ì„
        if "ì›ì‚°ì§€" in raw_data.columns:
            origins = raw_data["ì›ì‚°ì§€"].dropna()
            print(f"\nğŸ­ ì›ì‚°ì§€:")
            print(f"   - ê³ ìœ ê°’: {origins.nunique()}ê°œ")
            print(f"   - ìƒìœ„ 5ê°œ: {list(origins.value_counts().head().index)}")
            print(f"   - í•œêµ­ ê´€ë ¨: {len(origins[origins.str.contains('í•œêµ­|ëŒ€í•œë¯¼êµ­', na=False)])}ê±´")
        
        # í’ˆëª© ë¶„ì„
        if "í’ˆëª©" in raw_data.columns:
            items = raw_data["í’ˆëª©"].dropna()
            print(f"\nğŸ“¦ í’ˆëª©:")
            print(f"   - ê³ ìœ ê°’: {items.nunique()}ê°œ")
            print(f"   - ìƒìœ„ 5ê°œ: {list(items.value_counts().head().index)}")
            
            # í•œêµ­ ìˆ˜ì¶œ ê´€ë ¨ í’ˆëª© ì°¾ê¸°
            korean_export_keywords = ["ë¼ë©´", "ê¹€ì¹˜", "ì†Œì£¼", "ì „ì", "ìë™ì°¨", "ë°˜ë„ì²´", "í™”ì¥í’ˆ"]
            korean_items = []
            for keyword in korean_export_keywords:
                count = len(items[items.str.contains(keyword, na=False)])
                if count > 0:
                    korean_items.append(f"{keyword}({count}ê±´)")
            print(f"   - í•œêµ­ ìˆ˜ì¶œ ê´€ë ¨: {', '.join(korean_items)}")
        
        # ì¡°ì¹˜ì‚¬í•­ ë¶„ì„
        if "ì¡°ì¹˜ì‚¬í•­" in raw_data.columns:
            actions = raw_data["ì¡°ì¹˜ì‚¬í•­"].dropna()
            print(f"\nğŸ› ï¸ ì¡°ì¹˜ì‚¬í•­:")
            print(f"   - ê³ ìœ ê°’: {actions.nunique()}ê°œ")
            print(f"   - ìƒìœ„ 5ê°œ: {list(actions.value_counts().head().index)}")
        
        # ë¬¸ì œì‚¬ìœ  ë¶„ì„
        if "ë¬¸ì œì‚¬í•­" in raw_data.columns:
            reasons = raw_data["ë¬¸ì œì‚¬ìœ "].dropna()
            print(f"\nâŒ ë¬¸ì œì‚¬ìœ :")
            print(f"   - ê³ ìœ ê°’: {reasons.nunique()}ê°œ")
            print(f"   - í‰ê·  ê¸¸ì´: {reasons.str.len().mean():.1f}ì")
            
            # ì£¼ìš” ë¬¸ì œ í‚¤ì›Œë“œ ë¶„ì„
            problem_keywords = ["ì„œë¥˜", "ì¸ì¦", "ë¼ë²¨", "í‘œì‹œ", "ì„±ë¶„", "ê²€ì—­", "í•´ì¶©", "ë†ì•½", "ë°©ì‚¬ëŠ¥"]
            for keyword in problem_keywords:
                count = len(reasons[reasons.str.contains(keyword, na=False)])
                if count > 0:
                    print(f"   - '{keyword}' ê´€ë ¨: {count}ê±´")
        
        # 4. í•œêµ­ ìˆ˜ì¶œ ê´€ì ì—ì„œì˜ ë°ì´í„° ì í•©ì„±
        print(f"\nğŸ“‹ 4. í•œêµ­ ìˆ˜ì¶œ ê´€ì ì—ì„œì˜ ë°ì´í„° ì í•©ì„±")
        print("-" * 40)
        
        # í•œêµ­ì—ì„œ ìˆ˜ì¶œí•˜ëŠ” ê²½ìš° (í•œêµ­ì´ ì›ì‚°ì§€ì¸ ê²½ìš°)
        korean_origin = raw_data[raw_data["ì›ì‚°ì§€"].str.contains("í•œêµ­|ëŒ€í•œë¯¼êµ­", na=False)]
        print(f"ğŸ‡°ğŸ‡· í•œêµ­ ì›ì‚°ì§€ ë°ì´í„°: {len(korean_origin)}ê±´ ({len(korean_origin)/len(raw_data)*100:.1f}%)")
        
        if len(korean_origin) > 0:
            print(f"   - ì£¼ìš” ìˆ˜ì…êµ­: {list(korean_origin['ìˆ˜ì…êµ­'].value_counts().head(5).index)}")
            print(f"   - ì£¼ìš” í’ˆëª©: {list(korean_origin['í’ˆëª©'].value_counts().head(5).index)}")
        
        # í•œêµ­ìœ¼ë¡œ ìˆ˜ì…í•˜ëŠ” ê²½ìš° (í•œêµ­ì´ ìˆ˜ì…êµ­ì¸ ê²½ìš°)
        korean_import = raw_data[raw_data["ìˆ˜ì…êµ­"].str.contains("í•œêµ­|ëŒ€í•œë¯¼êµ­", na=False)]
        print(f"ğŸ“¥ í•œêµ­ ìˆ˜ì…êµ­ ë°ì´í„°: {len(korean_import)}ê±´ ({len(korean_import)/len(raw_data)*100:.1f}%)")
        
        # 5. ë°ì´í„° í’ˆì§ˆ í‰ê°€
        print(f"\nğŸ“‹ 5. ë°ì´í„° í’ˆì§ˆ í‰ê°€")
        print("-" * 40)
        
        # ì™„ì„±ë„ ì ìˆ˜ ê³„ì‚°
        completeness_scores = []
        for col in ["í’ˆëª©", "ì›ì‚°ì§€", "ìˆ˜ì…êµ­", "ì¡°ì¹˜ì‚¬í•­", "ë¬¸ì œì‚¬ìœ "]:
            if col in raw_data.columns:
                score = (len(raw_data) - raw_data[col].isnull().sum()) / len(raw_data)
                completeness_scores.append(score)
                print(f"   {col:<12}: {score:.1%}")
        
        avg_completeness = np.mean(completeness_scores) if completeness_scores else 0
        print(f"   í‰ê·  ì™„ì„±ë„: {avg_completeness:.1%}")
        
        # 6. ëª©ì  ì í•©ì„± í‰ê°€
        print(f"\nğŸ“‹ 6. ëª©ì  ì í•©ì„± í‰ê°€")
        print("-" * 40)
        
        # í•œêµ­ ìˆ˜ì¶œ ì§€ì› ê´€ì ì—ì„œì˜ í‰ê°€
        korean_export_relevance = len(korean_origin) / len(raw_data) * 100
        print(f"ğŸ‡°ğŸ‡· í•œêµ­ ìˆ˜ì¶œ ê´€ë ¨ì„±: {korean_export_relevance:.1f}%")
        
        if korean_export_relevance > 50:
            print("   âœ… í•œêµ­ ìˆ˜ì¶œ ì§€ì›ì— ë§¤ìš° ì í•©í•œ ë°ì´í„°")
        elif korean_export_relevance > 20:
            print("   âš ï¸ í•œêµ­ ìˆ˜ì¶œ ì§€ì›ì— ë¶€ë¶„ì ìœ¼ë¡œ ì í•©í•œ ë°ì´í„°")
        else:
            print("   âŒ í•œêµ­ ìˆ˜ì¶œ ì§€ì›ì— ë¶€ì í•©í•œ ë°ì´í„°")
        
        # ë°ì´í„° ë‹¤ì–‘ì„± í‰ê°€
        country_diversity = raw_data["ìˆ˜ì…êµ­"].nunique() if "ìˆ˜ì…êµ­" in raw_data.columns else 0
        item_diversity = raw_data["í’ˆëª©"].nunique() if "í’ˆëª©" in raw_data.columns else 0
        
        print(f"ğŸŒ êµ­ê°€ ë‹¤ì–‘ì„±: {country_diversity}ê°œ êµ­ê°€")
        print(f"ğŸ“¦ í’ˆëª© ë‹¤ì–‘ì„±: {item_diversity}ê°œ í’ˆëª©")
        
        # 7. ê°œì„  ì œì•ˆ
        print(f"\nğŸ“‹ 7. ê°œì„  ì œì•ˆ")
        print("-" * 40)
        
        if korean_export_relevance < 20:
            print("ğŸ’¡ í•œêµ­ ìˆ˜ì¶œ ê´€ë ¨ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            print("   - í•œêµ­ ì›ì‚°ì§€ ë°ì´í„° ìˆ˜ì§‘ í•„ìš”")
            print("   - í•œêµ­ ìˆ˜ì¶œ ì‹¤íŒ¨ ì‚¬ë¡€ ì¶”ê°€ í•„ìš”")
        
        if avg_completeness < 0.8:
            print("ğŸ’¡ ë°ì´í„° ì™„ì„±ë„ê°€ ë‚®ìŠµë‹ˆë‹¤.")
            print("   - ê²°ì¸¡ê°’ ë³´ì™„ í•„ìš”")
            print("   - ë°ì´í„° ì •ì œ ì‘ì—… í•„ìš”")
        
        if country_diversity < 10:
            print("ğŸ’¡ êµ­ê°€ ë‹¤ì–‘ì„±ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            print("   - ë” ë§ì€ ìˆ˜ì¶œëŒ€ìƒêµ­ ë°ì´í„° í•„ìš”")
        
        print(f"\nğŸ¯ ì¢…í•© í‰ê°€:")
        if korean_export_relevance > 30 and avg_completeness > 0.7 and country_diversity > 20:
            print("âœ… í•œêµ­ ìˆ˜ì¶œ ì§€ì› ëª©ì ì— ì í•©í•œ ë°ì´í„°")
        elif korean_export_relevance > 10 and avg_completeness > 0.5:
            print("âš ï¸ ë¶€ë¶„ì ìœ¼ë¡œ ì í•©í•˜ë‚˜ ê°œì„  í•„ìš”")
        else:
            print("âŒ ëª©ì ì— ë¶€ì í•©í•œ ë°ì´í„°")
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¶„ì„ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    analyze_data_quality() 