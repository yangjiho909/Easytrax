import os
import pickle
from glob import glob
import pandas as pd
from tqdm import tqdm
from sklearn.feature_extraction.text import TfidfVectorizer
from playwright.sync_api import sync_playwright
import time

def crawl_kati_with_playwright(save_dir="data"):
    print("ğŸŒ Playwrightë¡œ KATIì—ì„œ ë™ì ìœ¼ë¡œ ê²€ìƒ‰ í›„ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ì¤‘...")
    os.makedirs(save_dir, exist_ok=True)

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        page = browser.new_page()
        page.goto("https://www.kati.net/customs/customsList.do")
        time.sleep(2)  # JS ë¡œë”© ëŒ€ê¸°

        # ğŸ”§ ë°œìƒê¸°ê°„ ì„ íƒ
        page.select_option("select[name='fromYy']", value="2021")
        page.select_option("select[name='fromMm']", value="01")
        page.select_option("select[name='toYy']", value="2025")
        page.select_option("select[name='toMm']", value="12")

        # âœ… ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­
        page.click("button:has-text('ê²€ìƒ‰')")
        time.sleep(3)  # ê²°ê³¼ ë¡œë”© ëŒ€ê¸°

        # ğŸ“¥ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ í´ë¦­ (í‘œ í•˜ë‹¨ì— ìˆìŒ)
        links = page.query_selector_all("a[href$='.xlsx']")
        if not links:
            print("âŒ ì—‘ì…€ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return

        for i, link in enumerate(links[:10]):
            url = link.get_attribute("href")
            full_url = "https://www.kati.net" + url
            filename = f"customs_excel_{i}.xlsx"
            path = os.path.join(save_dir, filename)

            response = page.request.get(full_url)
            with open(path, "wb") as f:
                f.write(response.body())

            print(f"[âœ“] ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filename}")

        browser.close()


# ğŸ§¼ ì •ì œ + ë²¡í„°í™” + ëª¨ë¸ ì €ì¥
def train_and_save_model(data_dir="data", model_dir="model"):
    print("ğŸ§¼ í†µê´€ ì‹¤íŒ¨ ë°ì´í„° ì •ì œ ë° ëª¨ë¸ í•™ìŠµ...")
    excel_files = glob(os.path.join(data_dir, "customs_excel_*.xlsx"))
    all_df = []

    for file in tqdm(excel_files):
        try:
            df = pd.read_excel(file)
            sa_yu_cols = [col for col in df.columns if "ë¬¸ì œì‚¬ìœ " in col]
            df["ë¬¸ì œì‚¬ìœ "] = df[sa_yu_cols].fillna("").astype(str).agg(" ".join, axis=1)

            df = df[["í’ˆëª©", "ì›ì‚°ì§€", "ìˆ˜ì…êµ­", "ì¡°ì¹˜ì‚¬í•­", "ë¬¸ì œì‚¬ìœ "]].dropna()
            df["ì¶œì²˜íŒŒì¼"] = os.path.basename(file)
            all_df.append(df)
        except Exception as e:
            print(f"[!] {file} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

    if not all_df:
        raise ValueError("âŒ ìœ íš¨í•œ ë°ì´í„° ì—†ìŒ")

    full_df = pd.concat(all_df, ignore_index=True)
    full_df["í…ìŠ¤íŠ¸"] = (
        full_df["í’ˆëª©"].astype(str) + " " +
        full_df["ì›ì‚°ì§€"].astype(str) + " " +
        full_df["ìˆ˜ì…êµ­"].astype(str) + " " +
        full_df["ë¬¸ì œì‚¬ìœ "].astype(str)
    )

    vectorizer = TfidfVectorizer()
    X = vectorizer.fit_transform(full_df["í…ìŠ¤íŠ¸"])

    os.makedirs(model_dir, exist_ok=True)
    with open(os.path.join(model_dir, "vectorizer.pkl"), "wb") as f:
        pickle.dump(vectorizer, f)
    with open(os.path.join(model_dir, "indexed_matrix.pkl"), "wb") as f:
        pickle.dump(X, f)
    with open(os.path.join(model_dir, "raw_data.pkl"), "wb") as f:
        pickle.dump(full_df, f)

    print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ! ì´ ë°ì´í„° ìˆ˜: {len(full_df)}")

# ------------------------------
# ğŸ” ì™¸ë¶€ í˜¸ì¶œìš© í•¨ìˆ˜
# ------------------------------
def run_training():
    crawl_kati_with_playwright()
    train_and_save_model()
