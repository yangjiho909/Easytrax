#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ·ï¸ OCR ê¸°ë°˜ ë¼ë²¨ ì •ë³´ ì¶”ì¶œ ì‹œìŠ¤í…œ
- ë‹¤ì¤‘ OCR ì—”ì§„ ì§€ì› (Tesseract, EasyOCR)
- ì •ê·œí‘œí˜„ì‹ ê¸°ë°˜ ì •ë³´ ì¶”ì¶œ
- AI ê¸°ë°˜ NER ëª¨ë¸ í†µí•©
- ë°ì´í„° ì •ê·œí™” ë° ê²€ì¦
- í•œê¸€ ìš°ì„  ì¸ì‹ ë° ë‹¤êµ­ì–´ ë²ˆì—­ ì§€ì›
"""

import os
import re
import cv2
import numpy as np
from PIL import Image
import json
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
import logging

# OCR ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    import pytesseract
    TESSERACT_AVAILABLE = True
except ImportError:
    TESSERACT_AVAILABLE = False
    print("âš ï¸ Tesseractë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

try:
    import easyocr
    EASYOCR_AVAILABLE = True
except ImportError:
    EASYOCR_AVAILABLE = False
    print("âš ï¸ EasyOCRì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# ë²ˆì—­ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from deep_translator import GoogleTranslator
    TRANSLATOR_AVAILABLE = True
except ImportError:
    TRANSLATOR_AVAILABLE = False
    print("âš ï¸ Deep Translatorë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# AI/ML ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    import spacy
    SPACY_AVAILABLE = True
except ImportError:
    SPACY_AVAILABLE = False
    print("âš ï¸ spaCyë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

try:
    from transformers import pipeline
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("âš ï¸ Transformersë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# AI API í†µí•©
try:
    from ai_enhanced_ocr import ai_ocr
    AI_API_AVAILABLE = True
except ImportError:
    AI_API_AVAILABLE = False
    print("âš ï¸ AI APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

class LabelOCRExtractor:
    """OCR ê¸°ë°˜ ë¼ë²¨ ì •ë³´ ì¶”ì¶œê¸° (í•œê¸€ ìš°ì„  + ë²ˆì—­ ì§€ì›)"""
    
    def __init__(self):
        # ë¡œê¹… ì„¤ì • (ë¨¼ì € ì„¤ì •)
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # OCR ì—”ì§„ ì´ˆê¸°í™”
        self.ocr_engines = self._initialize_ocr_engines()
        self.ner_model = self._initialize_ner_model()
        self.extraction_patterns = self._load_extraction_patterns()
        self.data_normalizer = DataNormalizer()
        
        # ë²ˆì—­ê¸° ì´ˆê¸°í™”
        self.translator = self._initialize_translator()
        
        # AI API ì‚¬ìš© ì—¬ë¶€
        self.use_ai_apis = AI_API_AVAILABLE
    
    def _initialize_translator(self) -> Optional[Any]:
        """ë²ˆì—­ê¸° ì´ˆê¸°í™”"""
        if TRANSLATOR_AVAILABLE:
            try:
                # Deep TranslatorëŠ” ì¸ìŠ¤í„´ìŠ¤ ìƒì„±ì´ í•„ìš” ì—†ìŒ
                self.logger.info("âœ… Deep Translator ì´ˆê¸°í™” ì„±ê³µ")
                return True
            except Exception as e:
                self.logger.warning(f"âŒ Deep Translator ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None
    
    def _initialize_ocr_engines(self) -> Dict:
        """OCR ì—”ì§„ ì´ˆê¸°í™” (í•œê¸€ ìš°ì„ )"""
        engines = {}
        
        # Tesseract ì´ˆê¸°í™”
        if TESSERACT_AVAILABLE:
            try:
                # Tesseract ê²½ë¡œ ì„¤ì • (Windows)
                if os.name == 'nt':
                    # ì—¬ëŸ¬ ê°€ëŠ¥í•œ ê²½ë¡œ ì‹œë„
                    possible_paths = [
                        r'C:\Program Files\Tesseract-OCR\tesseract.exe',
                        r'C:\Program Files (x86)\Tesseract-OCR\tesseract.exe',
                        r'C:\Users\ì–‘ì§€í˜¸\AppData\Local\Programs\Tesseract-OCR\tesseract.exe'
                    ]
                    
                    tesseract_found = False
                    for path in possible_paths:
                        if os.path.exists(path):
                            pytesseract.pytesseract.tesseract_cmd = path
                            tesseract_found = True
                            break
                    
                    if not tesseract_found:
                        raise FileNotFoundError("Tesseract ì‹¤í–‰ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
                # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
                test_result = pytesseract.get_tesseract_version()
                engines['tesseract'] = {
                    'available': True,
                    'version': str(test_result),
                    'languages': ['kor', 'eng', 'chi_sim']  # í•œêµ­ì–´ ìš°ì„ 
                }
                self.logger.info(f"âœ… Tesseract ì´ˆê¸°í™” ì„±ê³µ: {test_result}")
            except Exception as e:
                engines['tesseract'] = {'available': False, 'error': str(e)}
                self.logger.warning(f"âŒ Tesseract ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # EasyOCR ì´ˆê¸°í™” (í•œêµ­ì–´ ìš°ì„ )
        if EASYOCR_AVAILABLE:
            try:
                # í•œêµ­ì–´ë¥¼ ì²« ë²ˆì§¸ ì–¸ì–´ë¡œ ì„¤ì •
                engines['easyocr'] = {
                    'available': True,
                    'languages': ['ko', 'en', 'ch_sim'],
                    'reader': easyocr.Reader(['ko', 'en', 'ch_sim'])  # í•œêµ­ì–´ ìš°ì„ 
                }
                self.logger.info("âœ… EasyOCR ì´ˆê¸°í™” ì„±ê³µ (í•œêµ­ì–´ ìš°ì„ )")
            except Exception as e:
                # ì‹¤íŒ¨ ì‹œ í•œêµ­ì–´ì™€ ì˜ì–´ë§Œ ì‚¬ìš©
                try:
                    engines['easyocr'] = {
                        'available': True,
                        'languages': ['ko', 'en'],
                        'reader': easyocr.Reader(['ko', 'en'])
                    }
                    self.logger.info("âœ… EasyOCR ì´ˆê¸°í™” ì„±ê³µ (í•œêµ­ì–´+ì˜ì–´)")
                except Exception as e2:
                    engines['easyocr'] = {'available': False, 'error': str(e2)}
                    self.logger.warning(f"âŒ EasyOCR ì´ˆê¸°í™” ì‹¤íŒ¨: {e2}")
        
        # ìµœì†Œ í•˜ë‚˜ì˜ OCR ì—”ì§„ì´ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
        available_engines = [name for name, config in engines.items() if config.get('available', False)]
        if not available_engines:
            self.logger.error("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ OCR ì—”ì§„ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            self.logger.info(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ OCR ì—”ì§„: {', '.join(available_engines)}")
        
        return engines
    
    def _initialize_ner_model(self) -> Optional[Any]:
        """NER ëª¨ë¸ ì´ˆê¸°í™”"""
        if SPACY_AVAILABLE:
            try:
                # í•œêµ­ì–´ ëª¨ë¸ ë¡œë“œ ì‹œë„
                try:
                    nlp = spacy.load("ko_core_news_sm")
                except OSError:
                    # í•œêµ­ì–´ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì˜ì–´ ëª¨ë¸ ì‚¬ìš©
                    nlp = spacy.load("en_core_web_sm")
                
                self.logger.info("âœ… spaCy NER ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ")
                return nlp
            except Exception as e:
                self.logger.warning(f"âŒ spaCy NER ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        if TRANSFORMERS_AVAILABLE:
            try:
                # Transformers ê¸°ë°˜ NER íŒŒì´í”„ë¼ì¸
                ner_pipeline = pipeline("ner", model="dslim/bert-base-NER")
                self.logger.info("âœ… Transformers NER ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ")
                return ner_pipeline
            except Exception as e:
                self.logger.warning(f"âŒ Transformers NER ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        return None
    
    def _load_extraction_patterns(self) -> Dict:
        """ì •ë³´ ì¶”ì¶œ íŒ¨í„´ ë¡œë”©"""
        return {
            # ì œí’ˆëª… íŒ¨í„´
            "product_name": [
                r"ì œí’ˆëª…[:\s]*([^\n\r]+)",
                r"ìƒí’ˆëª…[:\s]*([^\n\r]+)",
                r"Product[:\s]*([^\n\r]+)",
                r"Name[:\s]*([^\n\r]+)"
            ],
            
            # ì œì¡°ì‚¬ íŒ¨í„´
            "manufacturer": [
                r"ì œì¡°ì‚¬[:\s]*([^\n\r]+)",
                r"ì œì¡°ì›[:\s]*([^\n\r]+)",
                r"Manufacturer[:\s]*([^\n\r]+)",
                r"Made by[:\s]*([^\n\r]+)",
                r"Producer[:\s]*([^\n\r]+)"
            ],
            
            # ì›ì‚°ì§€ íŒ¨í„´
            "origin": [
                r"ì›ì‚°ì§€[:\s]*([^\n\r]+)",
                r"ì›ì‚°êµ­[:\s]*([^\n\r]+)",
                r"Origin[:\s]*([^\n\r]+)",
                r"Made in[:\s]*([^\n\r]+)"
            ],
            
            # ìœ í†µê¸°í•œ íŒ¨í„´
            "expiry_date": [
                r"ìœ í†µê¸°í•œ[:\s]*(\d{4}[-\/]\d{1,2}[-\/]\d{1,2})",
                r"ì‚¬ìš©ê¸°í•œ[:\s]*(\d{4}[-\/]\d{1,2}[-\/]\d{1,2})",
                r"Expiry[:\s]*(\d{4}[-\/]\d{1,2}[-\/]\d{1,2})",
                r"Best before[:\s]*(\d{4}[-\/]\d{1,2}[-\/]\d{1,2})",
                r"(\d{4}[-\/]\d{1,2}[-\/]\d{1,2})ê¹Œì§€"
            ],
            
            # ì˜ì–‘ì„±ë¶„ íŒ¨í„´
            "nutrition": {
                "calories": [
                    r"ì—´ëŸ‰[:\s]*(\d+(?:\.\d+)?)\s*(?:kcal|ì¹¼ë¡œë¦¬)",
                    r"Calories[:\s]*(\d+(?:\.\d+)?)\s*(?:kcal|cal)",
                    r"ì—ë„ˆì§€[:\s]*(\d+(?:\.\d+)?)\s*(?:kcal|ì¹¼ë¡œë¦¬)"
                ],
                "protein": [
                    r"ë‹¨ë°±ì§ˆ[:\s]*(\d+(?:\.\d+)?)\s*(?:g|ê·¸ë¨)",
                    r"Protein[:\s]*(\d+(?:\.\d+)?)\s*(?:g|gram)"
                ],
                "fat": [
                    r"ì§€ë°©[:\s]*(\d+(?:\.\d+)?)\s*(?:g|ê·¸ë¨)",
                    r"Fat[:\s]*(\d+(?:\.\d+)?)\s*(?:g|gram)"
                ],
                "carbohydrates": [
                    r"íƒ„ìˆ˜í™”ë¬¼[:\s]*(\d+(?:\.\d+)?)\s*(?:g|ê·¸ë¨)",
                    r"Carbohydrate[:\s]*(\d+(?:\.\d+)?)\s*(?:g|gram)"
                ],
                "sodium": [
                    r"ë‚˜íŠ¸ë¥¨[:\s]*(\d+(?:\.\d+)?)\s*(?:mg|ë°€ë¦¬ê·¸ë¨)",
                    r"Sodium[:\s]*(\d+(?:\.\d+)?)\s*(?:mg|milligram)"
                ],
                "sugar": [
                    r"ë‹¹ë¥˜[:\s]*(\d+(?:\.\d+)?)\s*(?:g|ê·¸ë¨)",
                    r"Sugar[:\s]*(\d+(?:\.\d+)?)\s*(?:g|gram)"
                ]
            },
            
            # ì„±ë¶„ íŒ¨í„´
            "ingredients": [
                r"ì„±ë¶„[:\s]*([^\n\r]+)",
                r"ì›ë£Œ[:\s]*([^\n\r]+)",
                r"Ingredients[:\s]*([^\n\r]+)",
                r"Contains[:\s]*([^\n\r]+)"
            ],
            
            # ì•Œë ˆë¥´ê¸° ì •ë³´ íŒ¨í„´
            "allergies": [
                r"ì•Œë ˆë¥´ê¸°[:\s]*([^\n\r]+)",
                r"ì•Œë ˆë¥´ê¸°ì„±ë¶„[:\s]*([^\n\r]+)",
                r"Allergy[:\s]*([^\n\r]+)",
                r"Contains[:\s]*([^\n\r]+)"
            ],
            
            # ë°”ì½”ë“œ íŒ¨í„´
            "barcode": [
                r"ë°”ì½”ë“œ[:\s]*(\d{13})",
                r"Barcode[:\s]*(\d{13})",
                r"(\d{13})",
                r"(\d{12})"
            ],
            
            # ì¤‘ëŸ‰ íŒ¨í„´
            "weight": [
                r"ì¤‘ëŸ‰[:\s]*(\d+(?:\.\d+)?)\s*(?:g|ml|kg|L)",
                r"Weight[:\s]*(\d+(?:\.\d+)?)\s*(?:g|ml|kg|L)",
                r"(\d+(?:\.\d+)?)\s*(?:g|ml|kg|L)"
            ]
        }
    
    def extract_label_info(self, image_path: str, use_advanced_ocr: bool = True, translate_to: str = None, use_ai_apis: bool = True) -> Dict:
        """ë¼ë²¨ ì´ë¯¸ì§€ì—ì„œ ì •ë³´ ì¶”ì¶œ (í•œê¸€ ìš°ì„  + ë²ˆì—­ ì§€ì› + AI API)"""
        
        self.logger.info(f"ğŸ” ë¼ë²¨ ì •ë³´ ì¶”ì¶œ ì‹œì‘: {image_path}")
        
        # AI API ìš°ì„  ì‚¬ìš© (ê°€ì¥ ì •í™•)
        if use_ai_apis and self.use_ai_apis:
            self.logger.info("ğŸ¤– AI API OCR ì‹œì‘...")
            ai_result = ai_ocr.extract_label_info(image_path, use_ai_apis=True)
            
            if ai_result:
                # AI API ê²°ê³¼ê°€ ìˆìœ¼ë©´ ë²ˆì—­ ì²˜ë¦¬
                if translate_to and self.translator:
                    self.logger.info(f"ğŸŒ AI ê²°ê³¼ ë²ˆì—­ ì‹œì‘: {translate_to}")
                    translated_info = self.translate_extracted_info(ai_result, translate_to)
                    ai_result = translated_info
                
                # ì‹ ë¢°ë„ í‰ê°€ (AI APIëŠ” ë†’ì€ ì‹ ë¢°ë„)
                confidence_scores = {key: 0.95 for key in ai_result.keys()}
                
                result = {
                    "extracted_info": ai_result,
                    "confidence_scores": confidence_scores,
                    "raw_text": "AI API ì¶”ì¶œ ê²°ê³¼",
                    "extraction_timestamp": datetime.now().isoformat(),
                    "image_path": image_path,
                    "translated": translate_to is not None,
                    "ai_enhanced": True
                }
                
                self.logger.info(f"âœ… AI API OCR ì™„ë£Œ: {len(ai_result)}ê°œ í•­ëª©")
                return result
        
        # AI APIê°€ ì—†ê±°ë‚˜ ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ì¡´ OCR ì‚¬ìš©
        self.logger.info("ğŸ“· ê¸°ì¡´ OCR ì—”ì§„ ì‚¬ìš©...")
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        processed_image = self._preprocess_image(image_path)
        
        # OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ (í•œê¸€ ìš°ì„ )
        extracted_text = self._extract_text(processed_image, use_advanced_ocr)
        
        # ì •ë³´ ì¶”ì¶œ
        label_info = self._extract_information(extracted_text)
        
        # ë°ì´í„° ì •ê·œí™”
        normalized_info = self.data_normalizer.normalize_data(label_info)
        
        # ë²ˆì—­ ì²˜ë¦¬ (ìš”ì²­ëœ ê²½ìš°)
        if translate_to and self.translator:
            self.logger.info(f"ğŸŒ ë²ˆì—­ ì‹œì‘: {translate_to}")
            translated_info = self.translate_extracted_info(normalized_info, translate_to)
            normalized_info = translated_info
        
        # ì‹ ë¢°ë„ í‰ê°€
        confidence_scores = self._calculate_confidence(extracted_text, normalized_info)
        
        result = {
            "extracted_info": normalized_info,
            "confidence_scores": confidence_scores,
            "raw_text": extracted_text,
            "extraction_timestamp": datetime.now().isoformat(),
            "image_path": image_path,
            "translated": translate_to is not None,
            "ai_enhanced": False
        }
        
        self.logger.info(f"âœ… ê¸°ì¡´ OCR ì™„ë£Œ: {len(normalized_info)}ê°œ í•­ëª©")
        return result
    
    def _preprocess_image(self, image_path: str) -> np.ndarray:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ê°•í™”"""
        try:
            # ì´ë¯¸ì§€ ë¡œë“œ
            image = cv2.imread(image_path)
            if image is None:
                raise ValueError(f"ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
            
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            # ë…¸ì´ì¦ˆ ì œê±°
            denoised = cv2.fastNlMeansDenoising(gray)
            
            # ëŒ€ë¹„ í–¥ìƒ
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
            enhanced = clahe.apply(denoised)
            
            # ì´ì§„í™” (ì ì‘í˜•)
            binary = cv2.adaptiveThreshold(
                enhanced, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                cv2.THRESH_BINARY, 11, 2
            )
            
            # ëª¨í´ë¡œì§€ ì—°ì‚°ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì„ ëª…í™”
            kernel = np.ones((1,1), np.uint8)
            processed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
            
            self.logger.info("âœ… ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì™„ë£Œ")
            return processed
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    
    def _extract_text(self, image: np.ndarray, use_advanced_ocr: bool = True) -> str:
        """OCRì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (í•œê¸€ ìš°ì„ )"""
        extracted_texts = []
        
        # Tesseract OCR (í•œêµ­ì–´ ìš°ì„ )
        if self.ocr_engines.get('tesseract', {}).get('available', False):
            try:
                # í•œêµ­ì–´ ìš°ì„  ì„¤ì •
                tesseract_config = '--oem 3 --psm 6 -l kor+eng+chi_sim'
                tesseract_text = pytesseract.image_to_string(image, config=tesseract_config)
                extracted_texts.append(('tesseract', tesseract_text))
                self.logger.info("âœ… Tesseract í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ (í•œêµ­ì–´ ìš°ì„ )")
            except Exception as e:
                self.logger.warning(f"âŒ Tesseract í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        # EasyOCR (í•œêµ­ì–´ ìš°ì„ )
        if self.ocr_engines.get('easyocr', {}).get('available', False):
            try:
                reader = self.ocr_engines['easyocr']['reader']
                easyocr_results = reader.readtext(image)
                easyocr_text = '\n'.join([text[1] for text in easyocr_results])
                extracted_texts.append(('easyocr', easyocr_text))
                self.logger.info("âœ… EasyOCR í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ (í•œêµ­ì–´ ìš°ì„ )")
            except Exception as e:
                self.logger.warning(f"âŒ EasyOCR í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        # OCR ì—”ì§„ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜
        if not extracted_texts:
            self.logger.warning("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ OCR ì—”ì§„ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return "OCR ì—”ì§„ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
        
        # í…ìŠ¤íŠ¸ ê²°í•© ë° ì •ë¦¬
        combined_text = self._combine_ocr_results(extracted_texts)
        
        # í•œê¸€ í…ìŠ¤íŠ¸ ì •ë¦¬ ë° ë²ˆì—­ ì¤€ë¹„
        cleaned_text = self._clean_korean_text(combined_text)
        
        return cleaned_text
    
    def _clean_korean_text(self, text: str) -> str:
        """í•œê¸€ í…ìŠ¤íŠ¸ ì •ë¦¬ ë° êµì • ê°•í™”"""
        if not text:
            return ""
        
        # ê¸°ë³¸ ì •ë¦¬
        cleaned = text.strip()
        
        # ì¼ë°˜ì ì¸ OCR ì˜¤ë¥˜ êµì •
        corrections = {
            'ì¤€íˆì–´': 'ì˜ì–‘ì„±ë¶„',
            'ë‚˜íŠ¸í†±': 'ë‚˜íŠ¸ë¥¨',
            'ODrig': '100g',
            'OOus': 'mg',
            'CCa': 'íƒ„ìˆ˜í™”ë¬¼',
            'GDa': 'ë‹¹ë¥˜',
            'J0us': 'ì§€ë°©',
            'GDg': 'íŠ¸ëœìŠ¤ì§€ë°©',
            'OICa': 'í¬í™”ì§€ë°©',
            'ì½œë ˆìŠ¤ë°ê°': 'ì½œë ˆìŠ¤í…Œë¡¤',
            'D079': '0.79',
            'OOu': 'mg',
            'ODE': 'ë‹¨ë°±ì§ˆ',
            '1ë§¥': '1íšŒ',
            'ì—¼lì‹¬ë¬¸': 'ì„­ì·¨ëŸ‰',
            'ê¸°ì¤€ì¹˜ë¨¸': 'ê¸°ì¤€ì¹˜ì—',
            'ë°”ë‹¥': 'ë¹„ìœ¨',
            'ê¼¬:': 'ëŒ€í•œ',
            'LJ(i': 'ë¹„ìœ¨',
            'í˜:': 'ëŒ€í•œ',
            'S 5Jg': '5g'
        }
        
        for wrong, correct in corrections.items():
            cleaned = cleaned.replace(wrong, correct)
        
        # ìˆ«ì íŒ¨í„´ ì •ë¦¬
        cleaned = re.sub(r'(\d+)\s*([a-zA-Zê°€-í£]+)', r'\1\2', cleaned)
        
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        cleaned = re.sub(r'\s+', ' ', cleaned)
        
        self.logger.info(f"ğŸ”§ í…ìŠ¤íŠ¸ êµì •: {text[:50]}... â†’ {cleaned[:50]}...")
        return cleaned
    
    def translate_text(self, text: str, target_lang: str = 'en') -> str:
        """í…ìŠ¤íŠ¸ ë²ˆì—­"""
        if not self.translator or not text.strip():
            return text
        
        try:
            # ì–¸ì–´ ì½”ë“œ ë§¤í•‘
            lang_mapping = {
                'en': 'en',
                'zh-cn': 'zh-cn',
                'ko': 'ko'
            }
            
            target_lang_code = lang_mapping.get(target_lang, 'en')
            
            # ë²ˆì—­ ì‹¤í–‰
            translator = GoogleTranslator(source='ko', target=target_lang_code)
            translated = translator.translate(text)
            self.logger.info(f"âœ… ë²ˆì—­ ì™„ë£Œ: {target_lang}")
            return translated
        except Exception as e:
            self.logger.warning(f"âŒ ë²ˆì—­ ì‹¤íŒ¨: {e}")
            return text
    
    def translate_extracted_info(self, extracted_info: Dict, target_lang: str = 'en') -> Dict:
        """ì¶”ì¶œëœ ì •ë³´ ë²ˆì—­"""
        if not self.translator:
            return extracted_info
        
        translated_info = extracted_info.copy()
        
        try:
            # ì œí’ˆëª… ë²ˆì—­
            if 'product_name' in translated_info and translated_info['product_name']:
                translated_info['product_name'] = self.translate_text(
                    translated_info['product_name'], target_lang
                )
            
            # ì œì¡°ì‚¬ ë²ˆì—­
            if 'manufacturer' in translated_info and translated_info['manufacturer']:
                translated_info['manufacturer'] = self.translate_text(
                    translated_info['manufacturer'], target_lang
                )
            
            # ì›ì‚°ì§€ ë²ˆì—­
            if 'origin' in translated_info and translated_info['origin']:
                translated_info['origin'] = self.translate_text(
                    translated_info['origin'], target_lang
                )
            
            # ì„±ë¶„ ì •ë³´ ë²ˆì—­
            if 'ingredients' in translated_info and translated_info['ingredients']:
                translated_info['ingredients'] = self.translate_text(
                    translated_info['ingredients'], target_lang
                )
            
            # ì•Œë ˆë¥´ê¸° ì •ë³´ ë²ˆì—­
            if 'allergies' in translated_info and translated_info['allergies']:
                if isinstance(translated_info['allergies'], list):
                    translated_allergies = []
                    for allergy in translated_info['allergies']:
                        translated_allergies.append(self.translate_text(allergy, target_lang))
                    translated_info['allergies'] = translated_allergies
                else:
                    translated_info['allergies'] = self.translate_text(
                        translated_info['allergies'], target_lang
                    )
            
            self.logger.info(f"âœ… ì¶”ì¶œ ì •ë³´ ë²ˆì—­ ì™„ë£Œ: {target_lang}")
            
        except Exception as e:
            self.logger.warning(f"âŒ ì¶”ì¶œ ì •ë³´ ë²ˆì—­ ì‹¤íŒ¨: {e}")
        
        return translated_info
    
    def _combine_ocr_results(self, ocr_results: List[Tuple[str, str]]) -> str:
        """OCR ê²°ê³¼ ê²°í•© ë° ì •ë¦¬"""
        if not ocr_results:
            return ""
        
        # ëª¨ë“  í…ìŠ¤íŠ¸ ê²°í•©
        all_texts = [text for _, text in ocr_results]
        combined = '\n'.join(all_texts)
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        cleaned_text = self._clean_text(combined)
        
        return cleaned_text
    
    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        
        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
        text = re.sub(r'[^\w\sê°€-í£\-\.\,\:\/\(\)]', '', text)
        
        # ì¤„ë°”ê¿ˆ ì •ë¦¬
        text = re.sub(r'\n+', '\n', text)
        
        return text.strip()
    
    def _extract_information(self, text: str) -> Dict:
        """ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•œ ì •ë³´ ì¶”ì¶œ"""
        extracted_info = {}
        
        # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        for field, patterns in self.extraction_patterns.items():
            if field == "nutrition":
                # ì˜ì–‘ì„±ë¶„ì€ ë³„ë„ ì²˜ë¦¬
                extracted_info[field] = self._extract_nutrition_info(text)
            else:
                # ì¼ë°˜ í•„ë“œ ì¶”ì¶œ
                for pattern in patterns:
                    match = re.search(pattern, text, re.IGNORECASE)
                    if match:
                        extracted_info[field] = match.group(1).strip()
                        break
        
        # NER ëª¨ë¸ì„ ì‚¬ìš©í•œ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
        if self.ner_model:
            ner_info = self._extract_with_ner(text)
            extracted_info.update(ner_info)
        
        return extracted_info
    
    def _extract_nutrition_info(self, text: str) -> Dict:
        """ì˜ì–‘ì„±ë¶„ ì •ë³´ ì¶”ì¶œ"""
        nutrition_info = {}
        
        for nutrient, patterns in self.extraction_patterns["nutrition"].items():
            for pattern in patterns:
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    nutrition_info[nutrient] = match.group(1).strip()
                    break
        
        return nutrition_info
    
    def _extract_with_ner(self, text: str) -> Dict:
        """NER ëª¨ë¸ì„ ì‚¬ìš©í•œ ì •ë³´ ì¶”ì¶œ"""
        ner_info = {}
        
        try:
            if hasattr(self.ner_model, 'pipe'):  # spaCy
                doc = self.ner_model(text)
                for ent in doc.ents:
                    if ent.label_ in ['ORG', 'PRODUCT', 'DATE', 'QUANTITY']:
                        ner_info[f"ner_{ent.label_.lower()}"] = ent.text
            elif hasattr(self.ner_model, '__call__'):  # Transformers
                ner_results = self.ner_model(text)
                for result in ner_results:
                    if result['score'] > 0.8:  # ì‹ ë¢°ë„ ì„ê³„ê°’
                        ner_info[f"ner_{result['entity_group'].lower()}"] = result['word']
        except Exception as e:
            self.logger.warning(f"NER ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return ner_info
    
    def _calculate_confidence(self, text: str, extracted_info: Dict) -> Dict:
        """ì¶”ì¶œ ì •ë³´ì˜ ì‹ ë¢°ë„ ê³„ì‚°"""
        confidence_scores = {}
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ë°˜ ê¸°ë³¸ ì‹ ë¢°ë„
        base_confidence = min(len(text) / 1000, 1.0)  # ìµœëŒ€ 1.0
        
        for field, value in extracted_info.items():
            if isinstance(value, dict):
                # ì˜ì–‘ì„±ë¶„ ë“± ì¤‘ì²©ëœ ì •ë³´
                confidence_scores[field] = {
                    k: self._calculate_field_confidence(v, text) 
                    for k, v in value.items()
                }
            else:
                # ì¼ë°˜ í•„ë“œ
                confidence_scores[field] = self._calculate_field_confidence(value, text)
        
        # ì „ì²´ ì‹ ë¢°ë„
        confidence_scores['overall'] = base_confidence
        
        return confidence_scores
    
    def _calculate_field_confidence(self, value: str, text: str) -> float:
        """ê°œë³„ í•„ë“œ ì‹ ë¢°ë„ ê³„ì‚°"""
        if not value:
            return 0.0
        
        # ê°’ì˜ ê¸¸ì´ì™€ ë³µì¡ì„± ê¸°ë°˜ ì‹ ë¢°ë„
        length_confidence = min(len(value) / 50, 1.0)
        
        # í…ìŠ¤íŠ¸ì—ì„œì˜ ë§¤ì¹­ ë¹ˆë„ ê¸°ë°˜ ì‹ ë¢°ë„
        match_count = text.lower().count(value.lower())
        frequency_confidence = min(match_count / 3, 1.0)
        
        # ìˆ«ì í¬í•¨ ì—¬ë¶€ (ì˜ì–‘ì„±ë¶„ ë“±)
        number_confidence = 1.0 if re.search(r'\d', value) else 0.5
        
        # ì¢…í•© ì‹ ë¢°ë„
        confidence = (length_confidence + frequency_confidence + number_confidence) / 3
        
        return round(confidence, 2)

class DataNormalizer:
    """ë°ì´í„° ì •ê·œí™” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.unit_mappings = {
            'g': 'g', 'ê·¸ë¨': 'g', 'gram': 'g', 'grams': 'g',
            'ml': 'ml', 'ë°€ë¦¬ë¦¬í„°': 'ml', 'milliliter': 'ml', 'milliliters': 'ml',
            'kg': 'kg', 'í‚¬ë¡œê·¸ë¨': 'kg', 'kilogram': 'kg', 'kilograms': 'kg',
            'kcal': 'kcal', 'ì¹¼ë¡œë¦¬': 'kcal', 'calorie': 'kcal', 'calories': 'kcal',
            'mg': 'mg', 'ë°€ë¦¬ê·¸ë¨': 'mg', 'milligram': 'mg', 'milligrams': 'mg'
        }
        
        self.date_patterns = [
            r'(\d{4})[-\/](\d{1,2})[-\/](\d{1,2})',
            r'(\d{1,2})[-\/](\d{1,2})[-\/](\d{4})',
            r'(\d{4})ë…„(\d{1,2})ì›”(\d{1,2})ì¼',
            r'(\d{1,2})ì›”(\d{1,2})ì¼(\d{4})ë…„'
        ]
    
    def normalize_data(self, data: Dict) -> Dict:
        """ë°ì´í„° ì •ê·œí™”"""
        normalized = {}
        
        for key, value in data.items():
            if isinstance(value, dict):
                normalized[key] = self.normalize_data(value)
            else:
                normalized[key] = self.normalize_field(key, value)
        
        return normalized
    
    def normalize_field(self, field: str, value: str) -> str:
        """ê°œë³„ í•„ë“œ ì •ê·œí™”"""
        if not value:
            return value
        
        # ë‹¨ìœ„ ì •ê·œí™”
        if field in ['weight', 'nutrition'] or any(unit in value.lower() for unit in self.unit_mappings):
            value = self.normalize_units(value)
        
        # ë‚ ì§œ ì •ê·œí™”
        if field in ['expiry_date', 'manufacture_date']:
            value = self.normalize_date(value)
        
        # ìˆ«ì ì •ê·œí™”
        if field in ['calories', 'protein', 'fat', 'carbohydrates', 'sodium', 'sugar']:
            value = self.normalize_number(value)
        
        return value
    
    def normalize_units(self, text: str) -> str:
        """ë‹¨ìœ„ ì •ê·œí™”"""
        for old_unit, new_unit in self.unit_mappings.items():
            text = re.sub(rf'\b{old_unit}\b', new_unit, text, flags=re.IGNORECASE)
        return text
    
    def normalize_date(self, date_str: str) -> str:
        """ë‚ ì§œ ì •ê·œí™”"""
        for pattern in self.date_patterns:
            match = re.search(pattern, date_str)
            if match:
                groups = match.groups()
                if len(groups) == 3:
                    if len(groups[0]) == 4:  # YYYY-MM-DD
                        return f"{groups[0]}-{groups[1].zfill(2)}-{groups[2].zfill(2)}"
                    else:  # MM-DD-YYYY
                        return f"{groups[2]}-{groups[0].zfill(2)}-{groups[1].zfill(2)}"
        return date_str
    
    def normalize_number(self, text: str) -> str:
        """ìˆ«ì ì •ê·œí™”"""
        # ìˆ«ìë§Œ ì¶”ì¶œ
        numbers = re.findall(r'\d+(?:\.\d+)?', text)
        if numbers:
            return numbers[0]
        return text

def main():
    """OCR ë¼ë²¨ ì¶”ì¶œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    
    print("ğŸ·ï¸ OCR ê¸°ë°˜ ë¼ë²¨ ì •ë³´ ì¶”ì¶œ ì‹œìŠ¤í…œ")
    print("=" * 50)
    
    extractor = LabelOCRExtractor()
    
    # OCR ì—”ì§„ ìƒíƒœ í™•ì¸
    print("\nğŸ“‹ OCR ì—”ì§„ ìƒíƒœ:")
    for engine_name, status in extractor.ocr_engines.items():
        if status.get('available', False):
            print(f"   âœ… {engine_name}: ì‚¬ìš© ê°€ëŠ¥")
            if 'version' in status:
                print(f"      ë²„ì „: {status['version']}")
        else:
            print(f"   âŒ {engine_name}: ì‚¬ìš© ë¶ˆê°€")
            if 'error' in status:
                print(f"      ì˜¤ë¥˜: {status['error']}")
    
    # NER ëª¨ë¸ ìƒíƒœ í™•ì¸
    if extractor.ner_model:
        print(f"   âœ… NER ëª¨ë¸: ì‚¬ìš© ê°€ëŠ¥")
    else:
        print(f"   âŒ NER ëª¨ë¸: ì‚¬ìš© ë¶ˆê°€")
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš° ì¶”ì¶œ í…ŒìŠ¤íŠ¸
    test_image_path = "test_label.png"
    if os.path.exists(test_image_path):
        print(f"\nğŸ” í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì¶”ì¶œ: {test_image_path}")
        try:
            result = extractor.extract_label_info(test_image_path)
            
            print(f"\nğŸ“„ ì¶”ì¶œëœ ì •ë³´:")
            for field, value in result['extracted_info'].items():
                if isinstance(value, dict):
                    print(f"   {field}:")
                    for sub_field, sub_value in value.items():
                        confidence = result['confidence_scores'][field].get(sub_field, 0)
                        print(f"     {sub_field}: {sub_value} (ì‹ ë¢°ë„: {confidence})")
                else:
                    confidence = result['confidence_scores'].get(field, 0)
                    print(f"   {field}: {value} (ì‹ ë¢°ë„: {confidence})")
            
            print(f"\nğŸ“Š ì „ì²´ ì‹ ë¢°ë„: {result['confidence_scores']['overall']}")
            
        except Exception as e:
            print(f"âŒ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    else:
        print(f"\nğŸ’¡ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì—¬ ì¶”ì¶œ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•´ë³´ì„¸ìš”.")

if __name__ == "__main__":
    main() 