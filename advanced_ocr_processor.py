#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ” ê³ ê¸‰ OCR ë° ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹œìŠ¤í…œ
- ë‹¤ì¤‘ OCR ì—”ì§„ ë³‘ë ¬ ì²˜ë¦¬
- ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (í•´ìƒë„ í–¥ìƒ, ì¡ìŒì œê±°, íšŒì „ ë³´ì •)
- í‘œ êµ¬ì¡° ë°ì´í„°í™” ë° ì•„ì´ì½˜/ì§ì¸ ì¶”ì¶œ
- ì‹ ë¢°ë„ ê¸°ë°˜ ê²°ê³¼ ê²€ì¦
"""

import os
import cv2
import numpy as np
import json
import base64
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from datetime import datetime
import logging

# ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from skimage import filters, restoration, transform, exposure
    from skimage.util import img_as_ubyte
    SKIMAGE_AVAILABLE = True
except ImportError:
    SKIMAGE_AVAILABLE = False
    print("âš ï¸ scikit-imageì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# OCR ì—”ì§„ë“¤
try:
    import easyocr
    EASYOCR_AVAILABLE = True
except ImportError:
    EASYOCR_AVAILABLE = False
    print("âš ï¸ EasyOCRì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

try:
    from paddleocr import PaddleOCR
    PADDLEOCR_AVAILABLE = True
except ImportError:
    PADDLEOCR_AVAILABLE = False
    print("âš ï¸ PaddleOCRì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

try:
    import pytesseract
    TESSERACT_AVAILABLE = True
except ImportError:
    TESSERACT_AVAILABLE = False
    print("âš ï¸ Tesseractë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# Google Cloud Vision
try:
    from google.cloud import vision
    GOOGLE_VISION_AVAILABLE = True
except ImportError:
    GOOGLE_VISION_AVAILABLE = False
    print("âš ï¸ Google Cloud Visionì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# Azure Computer Vision
try:
    from azure.cognitiveservices.vision.computervision import ComputerVisionClient
    from msrest.authentication import CognitiveServicesCredentials
    AZURE_VISION_AVAILABLE = True
except ImportError:
    AZURE_VISION_AVAILABLE = False
    print("âš ï¸ Azure Computer Visionì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

@dataclass
class OCRResult:
    """OCR ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    text: str
    confidence: float
    bbox: List[Tuple[int, int]]
    engine: str
    page: int = 1

@dataclass
class TableData:
    """í…Œì´ë¸” ë°ì´í„° í´ë˜ìŠ¤"""
    data: List[List[str]]
    bbox: List[Tuple[int, int]]
    confidence: float
    engine: str
    page: int = 1

@dataclass
class IconData:
    """ì•„ì´ì½˜/ì§ì¸ ë°ì´í„° í´ë˜ìŠ¤"""
    type: str  # 'icon', 'stamp', 'logo', 'signature'
    bbox: List[Tuple[int, int]]
    confidence: float
    engine: str
    image_data: Optional[bytes] = None
    page: int = 1

@dataclass
class LayoutData:
    """ë ˆì´ì•„ì›ƒ ë°ì´í„° í´ë˜ìŠ¤"""
    regions: List[Dict[str, Any]]
    confidence: float
    engine: str
    page: int = 1

class AdvancedImagePreprocessor:
    """ê³ ê¸‰ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ê¸° (ë¬¸ì„œë³„ ë§ì¶¤ ì„¤ì •)"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # ë¬¸ì„œë³„ ìµœì í™” ì„¤ì •
        self.document_settings = {
            'nutrition_label': {
                'enhance_resolution': True,
                'scale_factor': 2.5,
                'noise_reduction': 'strong',
                'contrast_enhancement': 'aggressive',
                'rotation_correction': True,
                'binarization': 'adaptive',
                'morphology': 'text_sharpening'
            },
            'customs_document': {
                'enhance_resolution': True,
                'scale_factor': 2.0,
                'noise_reduction': 'moderate',
                'contrast_enhancement': 'balanced',
                'rotation_correction': True,
                'binarization': 'otsu',
                'morphology': 'table_enhancement'
            },
            'general_document': {
                'enhance_resolution': False,
                'scale_factor': 1.5,
                'noise_reduction': 'light',
                'contrast_enhancement': 'conservative',
                'rotation_correction': True,
                'binarization': 'adaptive',
                'morphology': 'basic_cleaning'
            }
        }
    
    def preprocess_for_document_type(self, image: np.ndarray, document_type: str = 'general_document') -> Dict[str, Any]:
        """ë¬¸ì„œ ìœ í˜•ë³„ ë§ì¶¤ ì „ì²˜ë¦¬"""
        if document_type not in self.document_settings:
            document_type = 'general_document'
        
        settings = self.document_settings[document_type]
        self.logger.info(f"ğŸ”§ {document_type} ì „ì²˜ë¦¬ ì‹œì‘")
        
        processed_image = image.copy()
        preprocessing_info = {
            'document_type': document_type,
            'original_size': image.shape,
            'applied_settings': settings,
            'processing_steps': []
        }
        
        # 1. í•´ìƒë„ í–¥ìƒ
        if settings['enhance_resolution']:
            processed_image = self.enhance_resolution(processed_image, settings['scale_factor'])
            preprocessing_info['processing_steps'].append('resolution_enhancement')
        
        # 2. ì¡ìŒì œê±°
        if settings['noise_reduction'] != 'none':
            processed_image = self.remove_noise_optimized(processed_image, settings['noise_reduction'])
            preprocessing_info['processing_steps'].append('noise_reduction')
        
        # 3. ëŒ€ë¹„ í–¥ìƒ
        if settings['contrast_enhancement'] != 'none':
            processed_image = self.enhance_contrast_optimized(processed_image, settings['contrast_enhancement'])
            preprocessing_info['processing_steps'].append('contrast_enhancement')
        
        # 4. íšŒì „ ë³´ì •
        if settings['rotation_correction']:
            processed_image, rotation_angle = self.correct_rotation(processed_image)
            preprocessing_info['rotation_angle'] = rotation_angle
            preprocessing_info['processing_steps'].append('rotation_correction')
        
        # 5. ì´ì§„í™”
        if settings['binarization'] != 'none':
            processed_image = self.binarize_optimized(processed_image, settings['binarization'])
            preprocessing_info['processing_steps'].append('binarization')
        
        # 6. ëª¨í´ë¡œì§€ ì—°ì‚°
        if settings['morphology'] != 'none':
            processed_image = self.apply_morphology(processed_image, settings['morphology'])
            preprocessing_info['processing_steps'].append('morphology')
        
        preprocessing_info['final_size'] = processed_image.shape
        self.logger.info(f"âœ… {document_type} ì „ì²˜ë¦¬ ì™„ë£Œ: {len(preprocessing_info['processing_steps'])}ë‹¨ê³„")
        
        return {
            'processed_image': processed_image,
            'preprocessing_info': preprocessing_info
        }
    
    def enhance_resolution(self, image: np.ndarray, scale_factor: float = 2.0) -> np.ndarray:
        """í•´ìƒë„ í–¥ìƒ"""
        try:
            if SKIMAGE_AVAILABLE:
                # Lanczos ë³´ê°„ë²• ì‚¬ìš©
                enhanced = transform.rescale(image, scale_factor, order=3, anti_aliasing=True)
                return img_as_ubyte(enhanced)
            else:
                # OpenCV ì‚¬ìš©
                height, width = image.shape[:2]
                new_height, new_width = int(height * scale_factor), int(width * scale_factor)
                enhanced = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_LANCZOS4)
                return enhanced
        except Exception as e:
            self.logger.error(f"í•´ìƒë„ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return image
    
    def remove_noise(self, image: np.ndarray) -> np.ndarray:
        """ì¡ìŒ ì œê±°"""
        try:
            if SKIMAGE_AVAILABLE:
                # Non-local means denoising
                if len(image.shape) == 3:
                    denoised = restoration.denoise_nl_means(image, h=0.1, fast_mode=True)
                else:
                    denoised = restoration.denoise_nl_means(image, h=0.1, fast_mode=True)
                return img_as_ubyte(denoised)
            else:
                # OpenCV bilateral filter
                denoised = cv2.bilateralFilter(image, 9, 75, 75)
                return denoised
        except Exception as e:
            self.logger.error(f"ì¡ìŒ ì œê±° ì‹¤íŒ¨: {e}")
            return image
    
    def correct_rotation(self, image: np.ndarray) -> Tuple[np.ndarray, float]:
        """íšŒì „ ë³´ì •"""
        try:
            if len(image.shape) == 3:
                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            else:
                gray = image.copy()
            
            # ì´ì§„í™”
            _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
            
            # í…ìŠ¤íŠ¸ ë¼ì¸ ê²€ì¶œ
            lines = cv2.HoughLines(binary, 1, np.pi/180, threshold=100)
            
            if lines is not None:
                angles = []
                for rho, theta in lines[:10]:  # ìƒìœ„ 10ê°œ ë¼ì¸ë§Œ ì‚¬ìš©
                    angle = theta * 180 / np.pi
                    if angle < 45:
                        angles.append(angle)
                    elif angle > 135:
                        angles.append(angle - 180)
                
                if angles:
                    median_angle = np.median(angles)
                    if abs(median_angle) > 0.5:  # 0.5ë„ ì´ìƒì¼ ë•Œë§Œ íšŒì „
                        height, width = image.shape[:2]
                        center = (width // 2, height // 2)
                        rotation_matrix = cv2.getRotationMatrix2D(center, median_angle, 1.0)
                        corrected = cv2.warpAffine(image, rotation_matrix, (width, height))
                        return corrected, median_angle
            
            return image, 0.0
            
        except Exception as e:
            self.logger.error(f"íšŒì „ ë³´ì • ì‹¤íŒ¨: {e}")
            return image, 0.0
    
    def standardize_image(self, image: np.ndarray) -> np.ndarray:
        """ì´ë¯¸ì§€ í‘œì¤€í™”"""
        try:
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            if len(image.shape) == 3:
                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            else:
                gray = image.copy()
            
            # íˆìŠ¤í† ê·¸ë¨ í‰í™œí™”
            if SKIMAGE_AVAILABLE:
                equalized = exposure.equalize_hist(gray)
                return img_as_ubyte(equalized)
            else:
                equalized = cv2.equalizeHist(gray)
                return equalized
                
        except Exception as e:
            self.logger.error(f"ì´ë¯¸ì§€ í‘œì¤€í™” ì‹¤íŒ¨: {e}")
            return image
    
    def remove_noise_optimized(self, image: np.ndarray, intensity: str = 'moderate') -> np.ndarray:
        """ê°•ë„ë³„ ì¡ìŒì œê±°"""
        if not SKIMAGE_AVAILABLE:
            return image
        
        try:
            if intensity == 'strong':
                # ê°•í•œ ì¡ìŒì œê±° (ì˜ì–‘ì •ë³´ ë¼ë²¨ìš©)
                denoised = restoration.denoise_bilateral(image, sigma_color=0.1, sigma_spatial=15)
            elif intensity == 'moderate':
                # ì¤‘ê°„ ì¡ìŒì œê±° (í†µê´€ ì„œë¥˜ìš©)
                denoised = restoration.denoise_bilateral(image, sigma_color=0.05, sigma_spatial=10)
            else:  # light
                # ê°€ë²¼ìš´ ì¡ìŒì œê±° (ì¼ë°˜ ë¬¸ì„œìš©)
                denoised = restoration.denoise_bilateral(image, sigma_color=0.02, sigma_spatial=5)
            
            return img_as_ubyte(denoised)
        except Exception as e:
            self.logger.warning(f"âŒ ì¡ìŒì œê±° ì‹¤íŒ¨: {e}")
            return image
    
    def enhance_contrast_optimized(self, image: np.ndarray, intensity: str = 'balanced') -> np.ndarray:
        """ê°•ë„ë³„ ëŒ€ë¹„ í–¥ìƒ"""
        try:
            if intensity == 'aggressive':
                # ê³µê²©ì  ëŒ€ë¹„ í–¥ìƒ (ì˜ì–‘ì •ë³´ ë¼ë²¨ìš©)
                clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
                enhanced = clahe.apply(image)
            elif intensity == 'balanced':
                # ê· í˜•ì¡íŒ ëŒ€ë¹„ í–¥ìƒ (í†µê´€ ì„œë¥˜ìš©)
                clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
                enhanced = clahe.apply(image)
            else:  # conservative
                # ë³´ìˆ˜ì  ëŒ€ë¹„ í–¥ìƒ (ì¼ë°˜ ë¬¸ì„œìš©)
                clahe = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(8,8))
                enhanced = clahe.apply(image)
            
            return enhanced
        except Exception as e:
            self.logger.warning(f"âŒ ëŒ€ë¹„ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return image
    
    def binarize_optimized(self, image: np.ndarray, method: str = 'adaptive') -> np.ndarray:
        """ë°©ë²•ë³„ ì´ì§„í™”"""
        try:
            if method == 'adaptive':
                # ì ì‘í˜• ì´ì§„í™” (ëŒ€ë¶€ë¶„ì˜ ë¬¸ì„œì— ì í•©)
                binary = cv2.adaptiveThreshold(
                    image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                    cv2.THRESH_BINARY, 11, 2
                )
            elif method == 'otsu':
                # Otsu ì´ì§„í™” (ëª…í™•í•œ ëŒ€ë¹„ê°€ ìˆëŠ” ë¬¸ì„œì— ì í•©)
                _, binary = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
            else:  # simple
                # ë‹¨ìˆœ ì´ì§„í™”
                _, binary = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)
            
            return binary
        except Exception as e:
            self.logger.warning(f"âŒ ì´ì§„í™” ì‹¤íŒ¨: {e}")
            return image
    
    def apply_morphology(self, image: np.ndarray, operation: str = 'basic_cleaning') -> np.ndarray:
        """ëª¨í´ë¡œì§€ ì—°ì‚° ì ìš©"""
        try:
            if operation == 'text_sharpening':
                # í…ìŠ¤íŠ¸ ì„ ëª…í™” (ì˜ì–‘ì •ë³´ ë¼ë²¨ìš©)
                kernel = np.ones((1,1), np.uint8)
                sharpened = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)
                kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
                sharpened = cv2.filter2D(sharpened, -1, kernel)
            elif operation == 'table_enhancement':
                # í…Œì´ë¸” ê°•í™” (í†µê´€ ì„œë¥˜ìš©)
                kernel = np.ones((2,2), np.uint8)
                enhanced = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)
                kernel = np.ones((1,1), np.uint8)
                enhanced = cv2.morphologyEx(enhanced, cv2.MORPH_OPEN, kernel)
            else:  # basic_cleaning
                # ê¸°ë³¸ ì •ë¦¬ (ì¼ë°˜ ë¬¸ì„œìš©)
                kernel = np.ones((1,1), np.uint8)
                cleaned = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)
            
            return enhanced if operation == 'table_enhancement' else (sharpened if operation == 'text_sharpening' else cleaned)
        except Exception as e:
            self.logger.warning(f"âŒ ëª¨í´ë¡œì§€ ì—°ì‚° ì‹¤íŒ¨: {e}")
            return image
    
    def detect_document_type(self, image: np.ndarray) -> str:
        """ì´ë¯¸ì§€ì—ì„œ ë¬¸ì„œ ìœ í˜• ìë™ ê°ì§€"""
        try:
            # ì´ë¯¸ì§€ íŠ¹ì„± ë¶„ì„
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image
            
            # 1. í…ìŠ¤íŠ¸ ë°€ë„ ë¶„ì„
            edges = cv2.Canny(gray, 50, 150)
            text_density = np.sum(edges > 0) / (edges.shape[0] * edges.shape[1])
            
            # 2. ëŒ€ë¹„ ë¶„ì„
            contrast = np.std(gray)
            
            # 3. êµ¬ì¡°ì  ë³µì¡ë„ ë¶„ì„
            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
            
            # 4. ìƒ‰ìƒ ë¶„ì„ (ì˜ì–‘ì •ë³´ ë¼ë²¨ì€ ë³´í†µ ìƒ‰ìƒì´ ìˆìŒ)
            if len(image.shape) == 3:
                color_variance = np.std(image, axis=(0,1))
                has_color = np.any(color_variance > 30)
            else:
                has_color = False
            
            # ë¬¸ì„œ ìœ í˜• íŒë‹¨
            if has_color and text_density > 0.1 and contrast > 50:
                return 'nutrition_label'
            elif text_density > 0.15 and laplacian_var > 100:
                return 'customs_document'
            else:
                return 'general_document'
                
        except Exception as e:
            self.logger.warning(f"âŒ ë¬¸ì„œ ìœ í˜• ê°ì§€ ì‹¤íŒ¨: {e}")
            return 'general_document'
    
    def preprocess_image(self, image: np.ndarray, enhance_resolution: bool = True) -> Dict[str, Any]:
        """ì „ì²´ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        original = image.copy()
        processing_steps = []
        
        try:
            # 1. íšŒì „ ë³´ì •
            corrected, rotation_angle = self.correct_rotation(image)
            if abs(rotation_angle) > 0.5:
                processing_steps.append(f"íšŒì „ ë³´ì •: {rotation_angle:.2f}ë„")
                image = corrected
            
            # 2. ì¡ìŒ ì œê±°
            denoised = self.remove_noise(image)
            processing_steps.append("ì¡ìŒ ì œê±°")
            image = denoised
            
            # 3. í•´ìƒë„ í–¥ìƒ (ì„ íƒì )
            if enhance_resolution:
                enhanced = self.enhance_resolution(image, scale_factor=1.5)
                processing_steps.append("í•´ìƒë„ í–¥ìƒ (1.5ë°°)")
                image = enhanced
            
            # 4. í‘œì¤€í™”
            standardized = self.standardize_image(image)
            processing_steps.append("ì´ë¯¸ì§€ í‘œì¤€í™”")
            image = standardized
            
            return {
                'processed_image': image,
                'original_image': original,
                'processing_steps': processing_steps,
                'rotation_angle': rotation_angle
            }
            
        except Exception as e:
            self.logger.error(f"ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                'processed_image': original,
                'original_image': original,
                'processing_steps': ["ì „ì²˜ë¦¬ ì‹¤íŒ¨"],
                'rotation_angle': 0.0,
                'error': str(e)
            }

class MultiEngineOCRProcessor:
    """ë‹¤ì¤‘ OCR ì—”ì§„ ë³‘ë ¬ ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.preprocessor = AdvancedImagePreprocessor()
        
        # OCR ì—”ì§„ ì´ˆê¸°í™”
        self.engines = {}
        self._initialize_engines()
    
    def _initialize_engines(self):
        """OCR ì—”ì§„ ì´ˆê¸°í™”"""
        # EasyOCR
        if EASYOCR_AVAILABLE:
            try:
                self.engines['easyocr'] = easyocr.Reader(['ko', 'en', 'zh'])
                self.logger.info("âœ… EasyOCR ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                self.logger.error(f"âŒ EasyOCR ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # PaddleOCR
        if PADDLEOCR_AVAILABLE:
            try:
                self.engines['paddleocr'] = PaddleOCR(use_angle_cls=True, lang='korean')
                self.logger.info("âœ… PaddleOCR ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                self.logger.error(f"âŒ PaddleOCR ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # Tesseract
        if TESSERACT_AVAILABLE:
            try:
                # Tesseract ì„¤ì¹˜ í™•ì¸
                pytesseract.get_tesseract_version()
                self.engines['tesseract'] = 'tesseract'
                self.logger.info("âœ… Tesseract ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                self.logger.error(f"âŒ Tesseract ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # Google Cloud Vision
        if GOOGLE_VISION_AVAILABLE:
            try:
                self.engines['google_vision'] = vision.ImageAnnotatorClient()
                self.logger.info("âœ… Google Cloud Vision ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                self.logger.error(f"âŒ Google Cloud Vision ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # Azure Computer Vision
        if AZURE_VISION_AVAILABLE:
            try:
                endpoint = os.getenv('AZURE_VISION_ENDPOINT')
                key = os.getenv('AZURE_VISION_KEY')
                if endpoint and key:
                    self.engines['azure_vision'] = ComputerVisionClient(
                        endpoint, CognitiveServicesCredentials(key)
                    )
                    self.logger.info("âœ… Azure Computer Vision ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                self.logger.error(f"âŒ Azure Computer Vision ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _process_with_easyocr(self, image: np.ndarray) -> List[OCRResult]:
        """EasyOCRë¡œ ì²˜ë¦¬"""
        try:
            results = self.engines['easyocr'].readtext(image)
            ocr_results = []
            
            for (bbox, text, confidence) in results:
                ocr_results.append(OCRResult(
                    text=text,
                    confidence=confidence,
                    bbox=bbox,
                    engine='easyocr'
                ))
            
            return ocr_results
        except Exception as e:
            self.logger.error(f"EasyOCR ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return []
    
    def _process_with_paddleocr(self, image: np.ndarray) -> List[OCRResult]:
        """PaddleOCRë¡œ ì²˜ë¦¬"""
        try:
            results = self.engines['paddleocr'].ocr(image, cls=True)
            ocr_results = []
            
            if results and results[0]:
                for line in results[0]:
                    if line:
                        bbox, (text, confidence) = line
                        ocr_results.append(OCRResult(
                            text=text,
                            confidence=confidence,
                            bbox=bbox,
                            engine='paddleocr'
                        ))
            
            return ocr_results
        except Exception as e:
            self.logger.error(f"PaddleOCR ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return []
    
    def _process_with_tesseract(self, image: np.ndarray) -> List[OCRResult]:
        """Tesseractë¡œ ì²˜ë¦¬"""
        try:
            # Tesseract ì„¤ì •
            custom_config = r'--oem 3 --psm 6 -l kor+eng+chi_sim'
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = pytesseract.image_to_string(image, config=custom_config)
            
            # ë°”ìš´ë”© ë°•ìŠ¤ ì¶”ì¶œ
            data = pytesseract.image_to_data(image, config=custom_config, output_type=pytesseract.Output.DICT)
            
            ocr_results = []
            for i in range(len(data['text'])):
                if data['conf'][i] > 0:  # ì‹ ë¢°ë„ê°€ 0ë³´ë‹¤ í° ê²½ìš°ë§Œ
                    x, y, w, h = data['left'][i], data['top'][i], data['width'][i], data['height'][i]
                    bbox = [(x, y), (x + w, y), (x + w, y + h), (x, y + h)]
                    
                    ocr_results.append(OCRResult(
                        text=data['text'][i],
                        confidence=data['conf'][i] / 100.0,
                        bbox=bbox,
                        engine='tesseract'
                    ))
            
            return ocr_results
        except Exception as e:
            self.logger.error(f"Tesseract ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return []
    
    def _process_with_google_vision(self, image: np.ndarray) -> List[OCRResult]:
        """Google Cloud Visionìœ¼ë¡œ ì²˜ë¦¬"""
        try:
            # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
            _, buffer = cv2.imencode('.jpg', image)
            image_bytes = buffer.tobytes()
            
            # Vision API ìš”ì²­
            image_vision = vision.Image(content=image_bytes)
            response = self.engines['google_vision'].text_detection(image=image_vision)
            
            ocr_results = []
            if response.text_annotations:
                for annotation in response.text_annotations[1:]:  # ì²« ë²ˆì§¸ëŠ” ì „ì²´ í…ìŠ¤íŠ¸
                    vertices = [(vertex.x, vertex.y) for vertex in annotation.bounding_poly.vertices]
                    ocr_results.append(OCRResult(
                        text=annotation.description,
                        confidence=1.0,  # Google Visionì€ ì‹ ë¢°ë„ë¥¼ ì œê³µí•˜ì§€ ì•ŠìŒ
                        bbox=vertices,
                        engine='google_vision'
                    ))
            
            return ocr_results
        except Exception as e:
            self.logger.error(f"Google Cloud Vision ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return []
    
    def _process_with_azure_vision(self, image: np.ndarray) -> List[OCRResult]:
        """Azure Computer Visionìœ¼ë¡œ ì²˜ë¦¬"""
        try:
            # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
            _, buffer = cv2.imencode('.jpg', image)
            image_bytes = buffer.tobytes()
            
            # Azure Vision API ìš”ì²­
            response = self.engines['azure_vision'].read_in_stream(image_bytes, raw=True)
            operation_location = response.headers["Operation-Location"]
            operation_id = operation_location.split("/")[-1]
            
            # ê²°ê³¼ ëŒ€ê¸°
            import time
            while True:
                read_result = self.engines['azure_vision'].get_read_result(operation_id)
                if read_result.status not in ['notStarted', 'running']:
                    break
                time.sleep(1)
            
            ocr_results = []
            if read_result.status == "succeeded":
                for text_result in read_result.analyze_result.read_results:
                    for line in text_result.lines:
                        bbox = [(word.bounding_box[0], word.bounding_box[1]) for word in line.words]
                        text = ' '.join([word.text for word in line.words])
                        ocr_results.append(OCRResult(
                            text=text,
                            confidence=1.0,  # Azure Visionì€ ì‹ ë¢°ë„ë¥¼ ì œê³µí•˜ì§€ ì•ŠìŒ
                            bbox=bbox,
                            engine='azure_vision'
                        ))
            
            return ocr_results
        except Exception as e:
            self.logger.error(f"Azure Computer Vision ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return []
    
    def process_image_parallel(self, image: np.ndarray, document_type: str = None) -> Dict[str, Any]:
        """ì´ë¯¸ì§€ë¥¼ ë‹¤ì¤‘ OCR ì—”ì§„ìœ¼ë¡œ ë³‘ë ¬ ì²˜ë¦¬ (ë¬¸ì„œë³„ ë§ì¶¤ ì „ì²˜ë¦¬)"""
        self.logger.info("ğŸš€ ë‹¤ì¤‘ OCR ì—”ì§„ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘")
        
        # ë¬¸ì„œ ìœ í˜• ìë™ ê°ì§€ (ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš°)
        if document_type is None:
            document_type = self.preprocessor.detect_document_type(image)
            self.logger.info(f"ğŸ” ìë™ ê°ì§€ëœ ë¬¸ì„œ ìœ í˜•: {document_type}")
        
        # ë¬¸ì„œë³„ ë§ì¶¤ ì „ì²˜ë¦¬ ì ìš©
        preprocess_result = self.preprocessor.preprocess_for_document_type(image, document_type)
        processed_image = preprocess_result['processed_image']
        
        # ë³‘ë ¬ OCR ì²˜ë¦¬
        results = {}
        with ThreadPoolExecutor(max_workers=len(self.engines)) as executor:
            future_to_engine = {}
            
            for engine_name in self.engines.keys():
                if engine_name == 'easyocr':
                    future = executor.submit(self._process_with_easyocr, processed_image)
                elif engine_name == 'paddleocr':
                    future = executor.submit(self._process_with_paddleocr, processed_image)
                elif engine_name == 'tesseract':
                    future = executor.submit(self._process_with_tesseract, processed_image)
                elif engine_name == 'google_vision':
                    future = executor.submit(self._process_with_google_vision, processed_image)
                elif engine_name == 'azure_vision':
                    future = executor.submit(self._process_with_azure_vision, processed_image)
                
                future_to_engine[future] = engine_name
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for future in as_completed(future_to_engine):
                engine_name = future_to_engine[future]
                try:
                    results[engine_name] = future.result()
                    self.logger.info(f"âœ… {engine_name} ì²˜ë¦¬ ì™„ë£Œ: {len(results[engine_name])}ê°œ ê²°ê³¼")
                except Exception as e:
                    self.logger.error(f"âŒ {engine_name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    results[engine_name] = []
        
        # ê²°ê³¼ í†µí•© ë° ì‹ ë¢°ë„ ê¸°ë°˜ í•„í„°ë§
        integrated_results = self._integrate_results(results)
        
        # í…Œì´ë¸”, ì•„ì´ì½˜, ë ˆì´ì•„ì›ƒ ì¶”ì¶œ
        tables = self._extract_tables(integrated_results)
        icons = self._extract_icons(processed_image, integrated_results)
        layout = self._analyze_layout(integrated_results)
        
        self.logger.info(f"âœ… ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ: {len(integrated_results)}ê°œ í…ìŠ¤íŠ¸, {len(tables)}ê°œ í…Œì´ë¸”, {len(icons)}ê°œ ì•„ì´ì½˜")
        
        return {
            'text': [{'text': r.text, 'confidence': r.confidence, 'bbox': r.bbox, 'engine': r.engine} for r in integrated_results],
            'tables': [{'data': t.data, 'bbox': t.bbox, 'confidence': t.confidence, 'engine': t.engine} for t in tables],
            'icons': [{'type': i.type, 'bbox': i.bbox, 'confidence': i.confidence, 'engine': i.engine} for i in icons],
            'layout': {'regions': layout.regions, 'confidence': layout.confidence, 'engine': layout.engine},
            'preprocessing_info': preprocess_result['preprocessing_info'],
            'engine_performance': {name: len(results[name]) for name in results.keys()},
            'document_type': document_type
        }
    
    def _integrate_results(self, engine_results: Dict[str, List[OCRResult]]) -> List[OCRResult]:
        """ë‹¤ì¤‘ ì—”ì§„ ê²°ê³¼ í†µí•©"""
        all_results = []
        
        for engine_name, results in engine_results.items():
            for result in results:
                all_results.append(result)
        
        # ì‹ ë¢°ë„ ê¸°ë°˜ í•„í„°ë§ (0.3 ì´ìƒ)
        filtered_results = [r for r in all_results if r.confidence >= 0.3]
        
        # ì¤‘ë³µ ì œê±° (í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê¸°ë°˜)
        unique_results = self._remove_duplicates(filtered_results)
        
        return unique_results
    
    def _remove_duplicates(self, results: List[OCRResult]) -> List[OCRResult]:
        """ì¤‘ë³µ ê²°ê³¼ ì œê±°"""
        unique_results = []
        
        for result in results:
            is_duplicate = False
            for existing in unique_results:
                # í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°
                similarity = self._calculate_text_similarity(result.text, existing.text)
                if similarity > 0.8:  # 80% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼
                    # ë” ë†’ì€ ì‹ ë¢°ë„ë¥¼ ê°€ì§„ ê²°ê³¼ ì„ íƒ
                    if result.confidence > existing.confidence:
                        unique_results.remove(existing)
                        unique_results.append(result)
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_results.append(result)
        
        return unique_results
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        if not text1 or not text2:
            return 0.0
        
        # ê°„ë‹¨í•œ Jaccard ìœ ì‚¬ë„
        set1 = set(text1.lower())
        set2 = set(text2.lower())
        
        intersection = len(set1.intersection(set2))
        union = len(set1.union(set2))
        
        return intersection / union if union > 0 else 0.0
    
    def _extract_tables(self, ocr_results: List[OCRResult]) -> List[TableData]:
        """í…Œì´ë¸” êµ¬ì¡° ì¶”ì¶œ"""
        tables = []
        
        # í…ìŠ¤íŠ¸ë¥¼ ì¢Œí‘œ ê¸°ë°˜ìœ¼ë¡œ ì •ë ¬
        sorted_results = sorted(ocr_results, key=lambda x: (x.bbox[0][1], x.bbox[0][0]))
        
        # í…Œì´ë¸” íŒ¨í„´ ê²€ì¶œ
        table_regions = self._detect_table_regions(sorted_results)
        
        for region in table_regions:
            table_data = self._structure_table_data(region)
            if table_data:
                tables.append(table_data)
        
        return tables
    
    def _detect_table_regions(self, ocr_results: List[OCRResult]) -> List[List[OCRResult]]:
        """í…Œì´ë¸” ì˜ì—­ ê²€ì¶œ"""
        # ê°„ë‹¨í•œ í…Œì´ë¸” ê²€ì¶œ ë¡œì§
        # ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ì•Œê³ ë¦¬ì¦˜ì´ í•„ìš”
        regions = []
        current_region = []
        
        for result in ocr_results:
            if len(current_region) == 0:
                current_region.append(result)
            else:
                # ê°™ì€ í–‰ì— ìˆëŠ”ì§€ í™•ì¸
                last_result = current_region[-1]
                y_diff = abs(result.bbox[0][1] - last_result.bbox[0][1])
                
                if y_diff < 20:  # ê°™ì€ í–‰
                    current_region.append(result)
                else:
                    # ìƒˆë¡œìš´ í–‰ ì‹œì‘
                    if len(current_region) >= 3:  # ìµœì†Œ 3ê°œ ì´ìƒì˜ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ í…Œì´ë¸”ë¡œ ê°„ì£¼
                        regions.append(current_region)
                    current_region = [result]
        
        if len(current_region) >= 3:
            regions.append(current_region)
        
        return regions
    
    def _structure_table_data(self, region: List[OCRResult]) -> Optional[TableData]:
        """í…Œì´ë¸” ë°ì´í„° êµ¬ì¡°í™”"""
        try:
            # í–‰ë³„ë¡œ ê·¸ë£¹í™”
            rows = {}
            for result in region:
                y = result.bbox[0][1]
                row_key = y // 20  # 20í”½ì…€ ë‹¨ìœ„ë¡œ í–‰ ê·¸ë£¹í™”
                
                if row_key not in rows:
                    rows[row_key] = []
                rows[row_key].append(result)
            
            # ê° í–‰ì„ xì¢Œí‘œë¡œ ì •ë ¬
            table_data = []
            for row_key in sorted(rows.keys()):
                row_results = sorted(rows[row_key], key=lambda x: x.bbox[0][0])
                row_data = [result.text for result in row_results]
                table_data.append(row_data)
            
            if table_data:
                # ì „ì²´ ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°
                all_bboxes = [result.bbox for result in region]
                min_x = min([bbox[0][0] for bbox in all_bboxes])
                min_y = min([bbox[0][1] for bbox in all_bboxes])
                max_x = max([bbox[2][0] for bbox in all_bboxes])
                max_y = max([bbox[2][1] for bbox in all_bboxes])
                
                bbox = [(min_x, min_y), (max_x, min_y), (max_x, max_y), (min_x, max_y)]
                
                return TableData(
                    data=table_data,
                    bbox=bbox,
                    confidence=np.mean([result.confidence for result in region]),
                    engine='integrated'
                )
            
            return None
            
        except Exception as e:
            self.logger.error(f"í…Œì´ë¸” êµ¬ì¡°í™” ì‹¤íŒ¨: {e}")
            return None
    
    def _extract_icons(self, image: np.ndarray, ocr_results: List[OCRResult]) -> List[IconData]:
        """ì•„ì´ì½˜/ì§ì¸ ì¶”ì¶œ"""
        icons = []
        
        try:
            # í…ìŠ¤íŠ¸ ì˜ì—­ì„ ì œì™¸í•œ ì˜ì—­ì—ì„œ ì•„ì´ì½˜ ê²€ì¶œ
            text_mask = np.zeros(image.shape[:2], dtype=np.uint8)
            
            for result in ocr_results:
                bbox = result.bbox
                pts = np.array(bbox, np.int32)
                cv2.fillPoly(text_mask, [pts], 255)
            
            # í…ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ì˜ì—­
            non_text_mask = cv2.bitwise_not(text_mask)
            
            # ìœ¤ê³½ì„  ê²€ì¶œ
            contours, _ = cv2.findContours(non_text_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            for contour in contours:
                area = cv2.contourArea(contour)
                if area > 100:  # ìµœì†Œ í¬ê¸° í•„í„°
                    x, y, w, h = cv2.boundingRect(contour)
                    bbox = [(x, y), (x + w, y), (x + w, y + h), (x, y + h)]
                    
                    # ì•„ì´ì½˜ íƒ€ì… ë¶„ë¥˜
                    icon_type = self._classify_icon_type(image[y:y+h, x:x+w])
                    
                    # ì•„ì´ì½˜ ì´ë¯¸ì§€ ì¶”ì¶œ
                    icon_image = image[y:y+h, x:x+w]
                    _, icon_bytes = cv2.imencode('.png', icon_image)
                    
                    icons.append(IconData(
                        type=icon_type,
                        bbox=bbox,
                        confidence=0.8,  # ê¸°ë³¸ ì‹ ë¢°ë„
                        engine='opencv',
                        image_data=icon_bytes.tobytes()
                    ))
        
        except Exception as e:
            self.logger.error(f"ì•„ì´ì½˜ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return icons
    
    def _classify_icon_type(self, icon_image: np.ndarray) -> str:
        """ì•„ì´ì½˜ íƒ€ì… ë¶„ë¥˜"""
        try:
            # ê°„ë‹¨í•œ ë¶„ë¥˜ ë¡œì§
            # ì‹¤ì œë¡œëŠ” ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ì‚¬ìš©í•´ì•¼ í•¨
            
            # ì›í˜• ê²€ì¶œ (ì§ì¸)
            gray = cv2.cvtColor(icon_image, cv2.COLOR_BGR2GRAY)
            circles = cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, 1, 20, param1=50, param2=30, minRadius=0, maxRadius=0)
            
            if circles is not None:
                return 'stamp'
            
            # ì‚¬ê°í˜• ê²€ì¶œ (ë¡œê³ )
            _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
            contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            for contour in contours:
                area = cv2.contourArea(contour)
                if area > 50:
                    x, y, w, h = cv2.boundingRect(contour)
                    aspect_ratio = w / h
                    if 0.8 < aspect_ratio < 1.2:  # ì •ì‚¬ê°í˜•ì— ê°€ê¹Œìš°ë©´ ë¡œê³ 
                        return 'logo'
            
            return 'icon'
            
        except Exception as e:
            self.logger.error(f"ì•„ì´ì½˜ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return 'icon'
    
    def _analyze_layout(self, ocr_results: List[OCRResult]) -> LayoutData:
        """ë ˆì´ì•„ì›ƒ ë¶„ì„"""
        try:
            regions = []
            
            # í…ìŠ¤íŠ¸ ë¸”ë¡ ê·¸ë£¹í™”
            text_blocks = self._group_text_blocks(ocr_results)
            
            for block in text_blocks:
                region = {
                    'type': 'text',
                    'bbox': self._calculate_block_bbox(block),
                    'text': ' '.join([result.text for result in block]),
                    'confidence': np.mean([result.confidence for result in block])
                }
                regions.append(region)
            
            return LayoutData(
                regions=regions,
                confidence=0.9,
                engine='integrated'
            )
            
        except Exception as e:
            self.logger.error(f"ë ˆì´ì•„ì›ƒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return LayoutData(regions=[], confidence=0.0, engine='integrated')
    
    def _group_text_blocks(self, ocr_results: List[OCRResult]) -> List[List[OCRResult]]:
        """í…ìŠ¤íŠ¸ ë¸”ë¡ ê·¸ë£¹í™”"""
        blocks = []
        current_block = []
        
        for result in ocr_results:
            if not current_block:
                current_block.append(result)
            else:
                # ê°™ì€ ë¸”ë¡ì— ì†í•˜ëŠ”ì§€ í™•ì¸
                last_result = current_block[-1]
                distance = self._calculate_distance(result.bbox, last_result.bbox)
                
                if distance < 50:  # 50í”½ì…€ ì´ë‚´ë©´ ê°™ì€ ë¸”ë¡
                    current_block.append(result)
                else:
                    if current_block:
                        blocks.append(current_block)
                    current_block = [result]
        
        if current_block:
            blocks.append(current_block)
        
        return blocks
    
    def _calculate_distance(self, bbox1: List[Tuple[int, int]], bbox2: List[Tuple[int, int]]) -> float:
        """ë‘ ë°”ìš´ë”© ë°•ìŠ¤ ê°„ì˜ ê±°ë¦¬ ê³„ì‚°"""
        center1 = ((bbox1[0][0] + bbox1[2][0]) / 2, (bbox1[0][1] + bbox1[2][1]) / 2)
        center2 = ((bbox2[0][0] + bbox2[2][0]) / 2, (bbox2[0][1] + bbox2[2][1]) / 2)
        
        return np.sqrt((center1[0] - center2[0])**2 + (center1[1] - center2[1])**2)
    
    def _calculate_block_bbox(self, block: List[OCRResult]) -> List[Tuple[int, int]]:
        """ë¸”ë¡ì˜ ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°"""
        all_bboxes = [result.bbox for result in block]
        min_x = min([bbox[0][0] for bbox in all_bboxes])
        min_y = min([bbox[0][1] for bbox in all_bboxes])
        max_x = max([bbox[2][0] for bbox in all_bboxes])
        max_y = max([bbox[2][1] for bbox in all_bboxes])
        
        return [(min_x, min_y), (max_x, min_y), (max_x, max_y), (min_x, max_y)]

class OCRResultValidator:
    """OCR ê²°ê³¼ ê²€ì¦ ë° ì‚¬ìš©ì í™•ì¸ ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def validate_results(self, ocr_results: List[OCRResult], confidence_threshold: float = 0.5) -> Dict[str, Any]:
        """OCR ê²°ê³¼ ê²€ì¦"""
        validated_results = []
        low_confidence_results = []
        missing_items = []
        
        for result in ocr_results:
            if result.confidence >= confidence_threshold:
                validated_results.append(result)
            else:
                low_confidence_results.append(result)
        
        # ëˆ„ë½ëœ í•­ëª© ê²€ì¶œ (ê·œì œ ì¤€ìˆ˜ì„± ê´€ì ì—ì„œ)
        missing_items = self._detect_missing_items(validated_results)
        
        return {
            'validated_results': validated_results,
            'low_confidence_results': low_confidence_results,
            'missing_items': missing_items,
            'validation_summary': {
                'total_items': len(ocr_results),
                'validated_count': len(validated_results),
                'low_confidence_count': len(low_confidence_results),
                'missing_count': len(missing_items)
            }
        }
    
    def _detect_missing_items(self, validated_results: List[OCRResult]) -> List[str]:
        """ëˆ„ë½ëœ í•­ëª© ê²€ì¶œ"""
        missing_items = []
        
        # í•„ìˆ˜ ë¼ë²¨ë§ ìš”ì†Œë“¤
        required_elements = [
            'ì œí’ˆëª…', 'ì„±ë¶„', 'ì˜ì–‘ì„±ë¶„', 'ìœ í†µê¸°í•œ', 'ë³´ê´€ë°©ë²•',
            'ì œì¡°ì‚¬', 'ì›ì‚°ì§€', 'ì•Œë ˆë¥´ê¸°', 'ìš©ëŸ‰', 'ê°€ê²©'
        ]
        
        all_text = ' '.join([result.text for result in validated_results]).lower()
        
        for element in required_elements:
            if element.lower() not in all_text:
                missing_items.append(element)
        
        return missing_items
    
    def generate_user_interface_data(self, validation_result: Dict[str, Any]) -> Dict[str, Any]:
        """ì‚¬ìš©ì í™•ì¸ ì¸í„°í˜ì´ìŠ¤ ë°ì´í„° ìƒì„±"""
        return {
            'low_confidence_items': [
                {
                    'text': result.text,
                    'confidence': result.confidence,
                    'bbox': result.bbox,
                    'engine': result.engine,
                    'suggestions': self._generate_text_suggestions(result.text)
                }
                for result in validation_result['low_confidence_results']
            ],
            'missing_items': validation_result['missing_items'],
            'validation_summary': validation_result['validation_summary']
        }
    
    def _generate_text_suggestions(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ ìˆ˜ì • ì œì•ˆ ìƒì„±"""
        suggestions = []
        
        # ì¼ë°˜ì ì¸ OCR ì˜¤ë¥˜ íŒ¨í„´ ìˆ˜ì •
        common_errors = {
            '0': 'O',
            '1': 'l',
            '5': 'S',
            '8': 'B',
            'rn': 'm',
            'cl': 'd'
        }
        
        for error, correction in common_errors.items():
            if error in text:
                corrected = text.replace(error, correction)
                suggestions.append(corrected)
        
        return suggestions[:3]  # ìµœëŒ€ 3ê°œ ì œì•ˆ

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
advanced_ocr_processor = MultiEngineOCRProcessor()
ocr_validator = OCRResultValidator() 