#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ” ê°•í™”ëœ í‚¤ì›Œë“œ í™•ì¥ ì‹œìŠ¤í…œ
- ë™ì˜ì–´ ì‚¬ì „ í™•ì¥
- ì œí’ˆ ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ
- HS ì½”ë“œ ê¸°ë°˜ ì—°ê´€ í‚¤ì›Œë“œ
- ë‹¨ì–´ ë‹¨ìœ„ ìœ ì‚¬ë„ ê³„ì‚°
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from collections import defaultdict
import re
import pickle
import os

class EnhancedKeywordExpander:
    """ê°•í™”ëœ í‚¤ì›Œë“œ í™•ì¥ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.synonym_dict = self._load_synonym_dictionary()
        self.product_categories = self._load_product_categories()
        self.hs_code_keywords = self._load_hs_code_keywords()
        self.word_similarity_matrix = None
        self.word_vectorizer = None
        self._build_word_similarity_matrix()
    
    def _load_synonym_dictionary(self):
        """ë™ì˜ì–´ ì‚¬ì „ ë¡œë“œ"""
        return {
            # ë¼ë©´ ê´€ë ¨ ë™ì˜ì–´
            'ë¼ë©´': ['ë©´ë¥˜', 'ì¸ìŠ¤í„´íŠ¸', 'ì¦‰ì„', 'ì»µë¼ë©´', 'ë´‰ì§€ë¼ë©´', 'ìƒë©´', 'ê±´ë©´', 'ìš°ë™', 'ì†Œë°”', 'íŒŒìŠ¤íƒ€'],
            'ë©´ë¥˜': ['ë¼ë©´', 'ìš°ë™', 'ì†Œë°”', 'íŒŒìŠ¤íƒ€', 'ìŠ¤íŒŒê²Œí‹°', 'êµ­ìˆ˜', 'ëƒ‰ë©´', 'ì¹¼êµ­ìˆ˜'],
            'ì¸ìŠ¤í„´íŠ¸': ['ì¦‰ì„', 'ê°„í¸ì‹', 'ì¡°ë¦¬ì‹í’ˆ', 'ê°€ê³µì‹í’ˆ', 'ë ˆí† ë¥´íŠ¸'],
            
            # êµ­ê°€ ê´€ë ¨ ë™ì˜ì–´
            'ì¤‘êµ­': ['ì°¨ì´ë‚˜', 'ì¤‘í™”', 'ì¤‘í™”ì¸ë¯¼ê³µí™”êµ­', 'PRC', 'China'],
            'ë¯¸êµ­': ['USA', 'US', 'ì•„ë©”ë¦¬ì¹´', 'United States', 'ë¯¸í•©ì¤‘êµ­'],
            'í•œêµ­': ['ëŒ€í•œë¯¼êµ­', 'ROK', 'Korea', 'South Korea'],
            
            # ì œí’ˆ ìƒíƒœ ê´€ë ¨ ë™ì˜ì–´
            'ì‹ ì„ ': ['ìƒ', 'ëƒ‰ì¥', 'ëƒ‰ë™', 'ëƒ‰ê°', 'ì‹ ì„ ì‹í’ˆ'],
            'ê°€ê³µ': ['ì¡°ë¦¬', 'ì œì¡°', 'ê°€ê³µì‹í’ˆ', 'ì¡°ë¦¬ì‹í’ˆ'],
            'ê±´ì¡°': ['ë§ë¦°', 'ê±´ì¡°ì‹í’ˆ', 'ê±´ë©´', 'ê±´ì¡°ê³¼ì¼'],
            
            # ê±°ë¶€ ì‚¬ìœ  ê´€ë ¨ ë™ì˜ì–´
            'ìœ„ìƒ': ['ìœ„ìƒì¦ëª…ì„œ', 'ìœ„ìƒê²€ì‚¬', 'ìœ„ìƒê¸°ì¤€', 'ìœ„ìƒê·œì •'],
            'ê²€ì—­': ['ê²€ì—­ì¦ëª…ì„œ', 'ê²€ì—­ê²€ì‚¬', 'ê²€ì—­ê¸°ì¤€', 'ê²€ì—­ê·œì •'],
            'ì›ì‚°ì§€': ['ì›ì‚°ì§€ì¦ëª…ì„œ', 'ì›ì‚°ì§€í‘œì‹œ', 'ì›ì‚°ì§€ê¸°ì¤€'],
            'ì„±ë¶„': ['ì„±ë¶„ë¶„ì„', 'ì„±ë¶„í‘œì‹œ', 'ì„±ë¶„ê¸°ì¤€', 'ì˜ì–‘ì„±ë¶„'],
            'ì²¨ê°€ë¬¼': ['ì‹í’ˆì²¨ê°€ë¬¼', 'ë°©ë¶€ì œ', 'ìƒ‰ì†Œ', 'í–¥ë£Œ', 'ë³´ì¡´ë£Œ'],
            'ë†ì•½': ['ë†ì•½ì”ë¥˜', 'ë†ì•½ê²€ì‚¬', 'ë†ì•½ê¸°ì¤€', 'ë†ì•½ê·œì •'],
            'ì¤‘ê¸ˆì†': ['ë‚©', 'ì¹´ë“œë®´', 'ìˆ˜ì€', 'ë¹„ì†Œ', 'ì¤‘ê¸ˆì†ê²€ì‚¬'],
            'ë¯¸ìƒë¬¼': ['ì„¸ê· ', 'ë°”ì´ëŸ¬ìŠ¤', 'ê³°íŒ¡ì´', 'ë¯¸ìƒë¬¼ê²€ì‚¬', 'ë¯¸ìƒë¬¼ê¸°ì¤€'],
            'ë°©ì‚¬ëŠ¥': ['ë°©ì‚¬ëŠ¥ê²€ì‚¬', 'ë°©ì‚¬ëŠ¥ê¸°ì¤€', 'ë°©ì‚¬ëŠ¥ì˜¤ì—¼', 'ì„¸ìŠ˜', 'ìš”ì˜¤ë“œ'],
            
            # ì¡°ì¹˜ì‚¬í•­ ê´€ë ¨ ë™ì˜ì–´
            'ë°˜ì†¡': ['ë°˜ì¶œ', 'ë°˜í™˜', 'ì†¡í™˜', 'ë°˜ì†¡ì²˜ë¦¬'],
            'íê¸°': ['ì†Œê°', 'ë§¤ë¦½', 'íê¸°ì²˜ë¦¬', 'íê¸°ë¬¼ì²˜ë¦¬'],
            'ì¬ê²€ì‚¬': ['ì¬ê²€ì—­', 'ì¬ê²€ì‚¬', 'ì¶”ê°€ê²€ì‚¬', 'ì¬ì‹œí—˜'],
            'ìˆ˜ì •': ['ë³´ì™„', 'ìˆ˜ì •ì„œë¥˜', 'ë³´ì™„ì„œë¥˜', 'ì¬ì œì¶œ']
        }
    
    def _load_product_categories(self):
        """ì œí’ˆ ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ë¡œë“œ"""
        return {
            'ë©´ë¥˜': {
                'keywords': ['ë¼ë©´', 'ìš°ë™', 'ì†Œë°”', 'íŒŒìŠ¤íƒ€', 'ìŠ¤íŒŒê²Œí‹°', 'êµ­ìˆ˜', 'ëƒ‰ë©´', 'ì¹¼êµ­ìˆ˜', 'ë©´ë¥˜'],
                'hs_codes': ['1902', '1905', '1103', '1108'],
                'related_terms': ['ë°€ê°€ë£¨', 'ë©´', 'êµ­ìˆ˜', 'ì¡°ë¦¬ë©´', 'ê±´ë©´', 'ìƒë©´']
            },
            'ê³¼ì¼': {
                'keywords': ['ì‚¬ê³¼', 'ë°°', 'ë³µìˆ­ì•„', 'í¬ë„', 'ì˜¤ë Œì§€', 'ë°”ë‚˜ë‚˜', 'í‚¤ìœ„', 'ë§ê³ ', 'ê³¼ì¼'],
                'hs_codes': ['0801', '0802', '0803', '0804', '0805', '0806', '0807', '0808', '0809'],
                'related_terms': ['ì‹ ì„ ê³¼ì¼', 'ê±´ì¡°ê³¼ì¼', 'ëƒ‰ë™ê³¼ì¼', 'ê³¼ì¼ì£¼ìŠ¤', 'ê³¼ì¼ì¼']
            },
            'ì±„ì†Œ': {
                'keywords': ['ê³ ì¶”', 'ë§ˆëŠ˜', 'ì–‘íŒŒ', 'ë‹¹ê·¼', 'ì–‘ë°°ì¶”', 'ìƒì¶”', 'ì±„ì†Œ'],
                'hs_codes': ['0701', '0702', '0703', '0704', '0705', '0706', '0707', '0708', '0709'],
                'related_terms': ['ì‹ ì„ ì±„ì†Œ', 'ëƒ‰ë™ì±„ì†Œ', 'ê±´ì¡°ì±„ì†Œ', 'ì±„ì†Œì£¼ìŠ¤', 'ì±„ì†Œê°€ê³µí’ˆ']
            },
            'ìˆ˜ì‚°ë¬¼': {
                'keywords': ['ìƒì„ ', 'ìƒˆìš°', 'ê²Œ', 'ì¡°ê°œ', 'êµ´', 'ì „ë³µ', 'ìˆ˜ì‚°ë¬¼'],
                'hs_codes': ['0301', '0302', '0303', '0304', '0305', '0306', '0307'],
                'related_terms': ['ì‹ ì„ ìˆ˜ì‚°ë¬¼', 'ëƒ‰ë™ìˆ˜ì‚°ë¬¼', 'ê±´ì¡°ìˆ˜ì‚°ë¬¼', 'ìˆ˜ì‚°ê°€ê³µí’ˆ']
            },
            'ìœ¡ë¥˜': {
                'keywords': ['ë¼ì§€ê³ ê¸°', 'ì†Œê³ ê¸°', 'ë‹­ê³ ê¸°', 'ì–‘ê³ ê¸°', 'ìœ¡ë¥˜'],
                'hs_codes': ['0201', '0202', '0203', '0204', '0205', '0206', '0207'],
                'related_terms': ['ì‹ ì„ ìœ¡ë¥˜', 'ëƒ‰ë™ìœ¡ë¥˜', 'ìœ¡ê°€ê³µí’ˆ', 'ìœ¡ì œí’ˆ']
            },
            'ìœ ì œí’ˆ': {
                'keywords': ['ìš°ìœ ', 'ì¹˜ì¦ˆ', 'ë²„í„°', 'ìš”êµ¬ë¥´íŠ¸', 'ìœ ì œí’ˆ'],
                'hs_codes': ['0401', '0402', '0403', '0404', '0405', '0406'],
                'related_terms': ['ì‹ ì„ ìœ ì œí’ˆ', 'ëƒ‰ë™ìœ ì œí’ˆ', 'ìœ ê°€ê³µí’ˆ', 'ìœ ì œí’ˆ']
            }
        }
    
    def _load_hs_code_keywords(self):
        """HS ì½”ë“œ ê¸°ë°˜ ì—°ê´€ í‚¤ì›Œë“œ ë¡œë“œ"""
        return {
            # ë©´ë¥˜ ê´€ë ¨ HS ì½”ë“œ
            '1902': ['ë©´ë¥˜', 'íŒŒìŠ¤íƒ€', 'ìŠ¤íŒŒê²Œí‹°', 'ë¼ë©´', 'ìš°ë™', 'ì†Œë°”'],
            '1905': ['ë¹µ', 'ê³¼ì', 'ì œê³¼', 'ì œë¹µ', 'ë² ì´ì»¤ë¦¬'],
            '1103': ['ë°€ê°€ë£¨', 'ê³¡ë¬¼ê°€ë£¨', 'ë¶„ë§'],
            '1108': ['ì „ë¶„', 'ë…¹ë§', 'ê°€ê³µì „ë¶„'],
            
            # ê³¼ì¼ ê´€ë ¨ HS ì½”ë“œ
            '0801': ['ì½”ì½”ë„›', 'ë¸Œë¼ì§ˆë„›', 'ìºìŠˆë„›'],
            '0802': ['í˜¸ë‘', 'ê²¬ê³¼ë¥˜'],
            '0803': ['ë°”ë‚˜ë‚˜', 'ë°”ë‚˜ë‚˜ë¥˜'],
            '0804': ['ëŒ€ì¶”ì•¼ì', 'ë¬´í™”ê³¼', 'íŒŒì¸ì• í”Œ', 'ì•„ë³´ì¹´ë„'],
            '0805': ['ì˜¤ë Œì§€', 'ê°ê·¤ë¥˜'],
            '0806': ['í¬ë„', 'í¬ë„ë¥˜'],
            '0807': ['ë©œë¡ ', 'ìˆ˜ë°•', 'ê³¼ì‹¤ë¥˜'],
            '0808': ['ì‚¬ê³¼', 'ë°°', 'ì„ë¥˜'],
            '0809': ['ì‚´êµ¬', 'ì²´ë¦¬', 'ë³µìˆ­ì•„', 'ìë‘'],
            
            # ì±„ì†Œ ê´€ë ¨ HS ì½”ë“œ
            '0701': ['ê°ì', 'í† ë€', 'ë§ˆ', 'ê³ êµ¬ë§ˆ'],
            '0702': ['í† ë§ˆí† ', 'í† ë§ˆí† ë¥˜'],
            '0703': ['ì–‘íŒŒ', 'ë§ˆëŠ˜', 'ë¶€ì¶”', 'íŒŒ'],
            '0704': ['ì–‘ë°°ì¶”', 'ë¸Œë¡œì½œë¦¬', 'ì½œë¦¬í”Œë¼ì›Œ'],
            '0705': ['ìƒì¶”', 'ì¹˜ì»¤ë¦¬', 'ì—”ë””ë¸Œ'],
            '0706': ['ë‹¹ê·¼', 'ìˆœë¬´', 'ì‚¬íƒ•ë¬´'],
            '0707': ['ì˜¤ì´', 'í”¼í´ìš©ì˜¤ì´'],
            '0708': ['ì½©', 'ì™„ë‘ì½©'],
            '0709': ['ê¸°íƒ€ì±„ì†Œ', 'ì±„ì†Œë¥˜'],
            
            # ìˆ˜ì‚°ë¬¼ ê´€ë ¨ HS ì½”ë“œ
            '0301': ['ìƒì„ ', 'ì‹ ì„ ì–´ë¥˜'],
            '0302': ['ìƒì„ ', 'ëƒ‰ë™ì–´ë¥˜'],
            '0303': ['ìƒì„ ', 'ëƒ‰ë™ì–´ë¥˜'],
            '0304': ['ìƒì„ ', 'ëƒ‰ë™ì–´ë¥˜'],
            '0305': ['ìƒì„ ', 'ì—¼ì¥ì–´ë¥˜'],
            '0306': ['ê°‘ê°ë¥˜', 'ìƒˆìš°', 'ê²Œ'],
            '0307': ['ì—°ì²´ë™ë¬¼', 'ì¡°ê°œ', 'êµ´', 'ì „ë³µ']
        }
    
    def _build_word_similarity_matrix(self):
        """ë‹¨ì–´ ë‹¨ìœ„ ìœ ì‚¬ë„ í–‰ë ¬ êµ¬ì¶•"""
        try:
            # ëª¨ë“  í‚¤ì›Œë“œë¥¼ ìˆ˜ì§‘
            all_words = set()
            
            # ë™ì˜ì–´ ì‚¬ì „ì—ì„œ ë‹¨ì–´ ìˆ˜ì§‘
            for words in self.synonym_dict.values():
                all_words.update(words)
            
            # ì œí’ˆ ì¹´í…Œê³ ë¦¬ì—ì„œ ë‹¨ì–´ ìˆ˜ì§‘
            for category in self.product_categories.values():
                all_words.update(category['keywords'])
                all_words.update(category['related_terms'])
            
            # HS ì½”ë“œ í‚¤ì›Œë“œì—ì„œ ë‹¨ì–´ ìˆ˜ì§‘
            for words in self.hs_code_keywords.values():
                all_words.update(words)
            
            # ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            word_list = list(all_words)
            
            # TF-IDF ë²¡í„°í™”
            self.word_vectorizer = TfidfVectorizer(
                analyzer='char',  # ë¬¸ì ë‹¨ìœ„ ë¶„ì„
                ngram_range=(2, 4),  # 2-4ê¸€ì n-gram
                min_df=1
            )
            
            # ë‹¨ì–´ë“¤ì„ ë²¡í„°í™”
            word_vectors = self.word_vectorizer.fit_transform(word_list)
            
            # ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
            self.word_similarity_matrix = cosine_similarity(word_vectors)
            
            # ë‹¨ì–´-ì¸ë±ìŠ¤ ë§¤í•‘ ì €ì¥
            self.word_to_index = {word: idx for idx, word in enumerate(word_list)}
            self.index_to_word = {idx: word for idx, word in enumerate(word_list)}
            
            print(f"âœ… ë‹¨ì–´ ìœ ì‚¬ë„ í–‰ë ¬ êµ¬ì¶• ì™„ë£Œ: {len(word_list)}ê°œ ë‹¨ì–´")
            
        except Exception as e:
            print(f"âŒ ë‹¨ì–´ ìœ ì‚¬ë„ í–‰ë ¬ êµ¬ì¶• ì‹¤íŒ¨: {e}")
            self.word_similarity_matrix = None
    
    def calculate_word_similarity(self, word1, word2):
        """ë‘ ë‹¨ì–´ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        if self.word_similarity_matrix is None:
            return 0.0
        
        try:
            idx1 = self.word_to_index.get(word1)
            idx2 = self.word_to_index.get(word2)
            
            if idx1 is not None and idx2 is not None:
                return self.word_similarity_matrix[idx1][idx2]
            else:
                return 0.0
        except:
            return 0.0
    
    def find_similar_words(self, target_word, threshold=0.3, max_results=10):
        """ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ ì°¾ê¸°"""
        if self.word_similarity_matrix is None:
            return []
        
        try:
            target_idx = self.word_to_index.get(target_word)
            if target_idx is None:
                return []
            
            # ìœ ì‚¬ë„ ê³„ì‚°
            similarities = self.word_similarity_matrix[target_idx]
            
            # ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ ì°¾ê¸°
            similar_words = []
            for idx, similarity in enumerate(similarities):
                if idx != target_idx and similarity >= threshold:
                    similar_words.append({
                        'word': self.index_to_word[idx],
                        'similarity': similarity
                    })
            
            # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            similar_words.sort(key=lambda x: x['similarity'], reverse=True)
            return similar_words[:max_results]
            
        except Exception as e:
            print(f"âŒ ìœ ì‚¬ ë‹¨ì–´ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def expand_keywords_with_synonyms(self, keywords):
        """ë™ì˜ì–´ë¥¼ ì´ìš©í•œ í‚¤ì›Œë“œ í™•ì¥"""
        expanded = set(keywords)
        
        for keyword in keywords:
            if keyword in self.synonym_dict:
                expanded.update(self.synonym_dict[keyword])
        
        return list(expanded)
    
    def expand_keywords_with_categories(self, keywords):
        """ì œí’ˆ ì¹´í…Œê³ ë¦¬ë¥¼ ì´ìš©í•œ í‚¤ì›Œë“œ í™•ì¥"""
        expanded = set(keywords)
        
        for keyword in keywords:
            for category_name, category_info in self.product_categories.items():
                if keyword in category_info['keywords']:
                    expanded.update(category_info['keywords'])
                    expanded.update(category_info['related_terms'])
                    break
        
        return list(expanded)
    
    def expand_keywords_with_hs_codes(self, keywords):
        """HS ì½”ë“œë¥¼ ì´ìš©í•œ í‚¤ì›Œë“œ í™•ì¥"""
        expanded = set(keywords)
        
        for keyword in keywords:
            for hs_code, hs_keywords in self.hs_code_keywords.items():
                if keyword in hs_keywords:
                    expanded.update(hs_keywords)
                    break
        
        return list(expanded)
    
    def expand_keywords_with_similarity(self, keywords, threshold=0.3):
        """ìœ ì‚¬ë„ ê¸°ë°˜ í‚¤ì›Œë“œ í™•ì¥"""
        expanded = set(keywords)
        
        for keyword in keywords:
            similar_words = self.find_similar_words(keyword, threshold)
            for similar_word in similar_words:
                expanded.add(similar_word['word'])
        
        return list(expanded)
    
    def enhanced_expand_keywords(self, user_input, use_synonyms=True, use_categories=True, 
                               use_hs_codes=True, use_similarity=True, similarity_threshold=0.3):
        """í†µí•© í‚¤ì›Œë“œ í™•ì¥"""
        # ì…ë ¥ì„ ë‹¨ì–´ë¡œ ë¶„ë¦¬
        words = user_input.split()
        expanded_words = set(words)
        
        if use_synonyms:
            expanded_words.update(self.expand_keywords_with_synonyms(words))
        
        if use_categories:
            expanded_words.update(self.expand_keywords_with_categories(words))
        
        if use_hs_codes:
            expanded_words.update(self.expand_keywords_with_hs_codes(words))
        
        if use_similarity:
            expanded_words.update(self.expand_keywords_with_similarity(words, similarity_threshold))
        
        # í™•ì¥ëœ í‚¤ì›Œë“œë“¤ì„ ê³µë°±ìœ¼ë¡œ ì—°ê²°
        expanded_input = ' '.join(expanded_words)
        
        return expanded_input, list(expanded_words)
    
    def get_expansion_info(self, user_input):
        """í‚¤ì›Œë“œ í™•ì¥ ì •ë³´ ë°˜í™˜"""
        original_words = user_input.split()
        
        expansion_info = {
            'original_input': user_input,
            'original_words': original_words,
            'expansions': {}
        }
        
        # ë™ì˜ì–´ í™•ì¥
        synonym_expanded = self.expand_keywords_with_synonyms(original_words)
        expansion_info['expansions']['synonyms'] = {
            'words': synonym_expanded,
            'count': len(synonym_expanded)
        }
        
        # ì¹´í…Œê³ ë¦¬ í™•ì¥
        category_expanded = self.expand_keywords_with_categories(original_words)
        expansion_info['expansions']['categories'] = {
            'words': category_expanded,
            'count': len(category_expanded)
        }
        
        # HS ì½”ë“œ í™•ì¥
        hs_expanded = self.expand_keywords_with_hs_codes(original_words)
        expansion_info['expansions']['hs_codes'] = {
            'words': hs_expanded,
            'count': len(hs_expanded)
        }
        
        # ìœ ì‚¬ë„ í™•ì¥
        similarity_expanded = self.expand_keywords_with_similarity(original_words)
        expansion_info['expansions']['similarity'] = {
            'words': similarity_expanded,
            'count': len(similarity_expanded)
        }
        
        return expansion_info

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    expander = EnhancedKeywordExpander()
    
    # í…ŒìŠ¤íŠ¸
    test_input = "ì¤‘êµ­ ë¼ë©´"
    expanded_input, expanded_words = expander.enhanced_expand_keywords(test_input)
    
    print(f"ì›ë³¸ ì…ë ¥: {test_input}")
    print(f"í™•ì¥ëœ ì…ë ¥: {expanded_input}")
    print(f"í™•ì¥ëœ ë‹¨ì–´ ìˆ˜: {len(expanded_words)}")
    
    # í™•ì¥ ì •ë³´ ì¶œë ¥
    expansion_info = expander.get_expansion_info(test_input)
    print("\ní™•ì¥ ì •ë³´:")
    for method, info in expansion_info['expansions'].items():
        print(f"{method}: {info['count']}ê°œ ë‹¨ì–´")
    
    # ìœ ì‚¬ ë‹¨ì–´ í…ŒìŠ¤íŠ¸
    print(f"\n'ë¼ë©´'ê³¼ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤:")
    similar_words = expander.find_similar_words('ë¼ë©´', threshold=0.2)
    for word_info in similar_words[:5]:
        print(f"  {word_info['word']}: {word_info['similarity']:.3f}") 