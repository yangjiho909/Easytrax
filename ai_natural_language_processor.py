"""
ë¬´ë£Œ AI ì„œë¹„ìŠ¤ë¥¼ í™œìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ëª¨ë“ˆ
Hugging Face, OpenAI API (ë¬´ë£Œ í‹°ì–´), ë¡œì»¬ ëª¨ë¸ ë“±ì„ í™œìš©
"""

import json
import logging
import requests
import time
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import re
import os

logger = logging.getLogger(__name__)

@dataclass
class AIProcessedQuery:
    """AI ì²˜ë¦¬ëœ ì§ˆì˜ ê²°ê³¼"""
    original_query: str
    processed_query: str
    intent: str
    entities: Dict[str, Any]
    confidence: float
    ai_service_used: str

class FreeAINaturalLanguageProcessor:
    """ë¬´ë£Œ AI ì„œë¹„ìŠ¤ë¥¼ í™œìš©í•œ ìì—°ì–´ ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        self.huggingface_api_key = os.getenv('HUGGINGFACE_API_KEY', '')
        self.openai_api_key = os.getenv('OPENAI_API_KEY', '')
        
        # ë¬´ë£Œ AI ì„œë¹„ìŠ¤ ì„¤ì •
        self.services = {
            'huggingface': {
                'enabled': bool(self.huggingface_api_key),
                'url': 'https://api-inference.huggingface.co/models/',
                'models': {
                    'text_classification': 'facebook/bart-large-mnli',
                    'question_answering': 'deepset/roberta-base-squad2',
                    'text_generation': 'gpt2'
                }
            },
            'openai': {
                'enabled': bool(self.openai_api_key),
                'url': 'https://api.openai.com/v1/',
                'models': {
                    'chat': 'gpt-3.5-turbo',
                    'completion': 'text-davinci-003'
                }
            }
        }
        
        # ë¡œì»¬ ê·œì¹™ ê¸°ë°˜ ì²˜ë¦¬
        self.intent_patterns = {
            'regulation': [
                r'ê·œì œ|ì„œë¥˜|í•„ìš”|ìš”êµ¬|ì¤€ìˆ˜|ë²•ê·œ|ë²•ë ¹',
                r'ì¤‘êµ­.*ë¼ë©´|ë¯¸êµ­.*ë¼ë©´|ìˆ˜ì¶œ.*ê·œì œ|FDA|GB'
            ],
            'statistics': [
                r'í†µê³„|ìˆ˜ì¹˜|ë°ì´í„°|ë¹„êµ|ë¶„ì„|ì¶”ì´|ì„±ì¥ë¥ |ì ìœ ìœ¨',
                r'HSì½”ë“œ|ìˆ˜ì¶œ.*í†µê³„|ìˆ˜ì….*í†µê³„|ë¬´ì—­.*í†µê³„'
            ],
            'market_analysis': [
                r'ì‹œì¥|ë™í–¥|ì „ë§|ì„±ì¥|ë¦¬ìŠ¤í¬|ê¸°íšŒ|ê²½ìŸ',
                r'ì‹œì¥.*ë¶„ì„|ë™í–¥.*ë¶„ì„|ì „ëµ.*ë³´ê³ ì„œ'
            ],
            'documentation': [
                r'ì„œë¥˜|ë¬¸ì„œ|ì¸ì¦ì„œ|ì¦ëª…ì„œ|ì‹ ê³ ì„œ|ì‹ ì²­ì„œ',
                r'í•„ìš”.*ì„œë¥˜|ì¤€ë¹„.*ì„œë¥˜|ì œì¶œ.*ì„œë¥˜'
            ]
        }
        
        self.entity_patterns = {
            'country': [
                r'ì¤‘êµ­|ì°¨ì´ë‚˜|ì¤‘í™”',
                r'ë¯¸êµ­|USA|US|ì•„ë©”ë¦¬ì¹´'
            ],
            'product': [
                r'ë¼ë©´|ë©´ë¥˜|ì¸ìŠ¤í„´íŠ¸|ì¦‰ì„',
                r'ì‹í’ˆ|ìŒì‹|ê°€ê³µì‹í’ˆ'
            ],
            'hs_code': [
                r'HSì½”ë“œ|HS.*ì½”ë“œ|ì½”ë“œ.*\d{6}',
                r'\d{6}.*ì½”ë“œ'
            ],
            'time_period': [
                r'\d{4}ë…„|\d{4}ë…„.*ë¶„ê¸°|\d{4}ë…„.*ì›”',
                r'ì˜¬í•´|ì‘ë…„|ë‚´ë…„|ìµœê·¼|í˜„ì¬'
            ]
        }
    
    def process_query(self, query: str) -> AIProcessedQuery:
        """ì§ˆì˜ë¥¼ AIë¡œ ì²˜ë¦¬í•˜ì—¬ êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ"""
        try:
            # 1. ì˜ë„ ë¶„ì„
            intent = self._analyze_intent(query)
            
            # 2. ì—”í‹°í‹° ì¶”ì¶œ
            entities = self._extract_entities(query)
            
            # 3. AI ì„œë¹„ìŠ¤ë¥¼ í†µí•œ ê³ ê¸‰ ì²˜ë¦¬
            processed_query = self._enhance_with_ai(query, intent, entities)
            
            # 4. ì‹ ë¢°ë„ ê³„ì‚°
            confidence = self._calculate_confidence(query, intent, entities)
            
            # 5. ì‚¬ìš©ëœ AI ì„œë¹„ìŠ¤ ê²°ì •
            ai_service = self._determine_ai_service()
            
            return AIProcessedQuery(
                original_query=query,
                processed_query=processed_query,
                intent=intent,
                entities=entities,
                confidence=confidence,
                ai_service_used=ai_service
            )
            
        except Exception as e:
            logger.error(f"AI ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            # í´ë°±: ê¸°ë³¸ ê·œì¹™ ê¸°ë°˜ ì²˜ë¦¬
            return self._fallback_processing(query)
    
    def _analyze_intent(self, query: str) -> str:
        """ì˜ë„ ë¶„ì„"""
        query_lower = query.lower()
        
        # AI ì„œë¹„ìŠ¤ í™œìš© (ê°€ëŠ¥í•œ ê²½ìš°)
        if self.services['huggingface']['enabled']:
            try:
                return self._huggingface_intent_analysis(query)
            except:
                pass
        
        # ê·œì¹™ ê¸°ë°˜ ë¶„ì„
        scores = {}
        for intent, patterns in self.intent_patterns.items():
            score = 0
            for pattern in patterns:
                if re.search(pattern, query_lower):
                    score += 1
            scores[intent] = score
        
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì˜ë„ ë°˜í™˜
        if scores:
            return max(scores, key=scores.get)
        
        return 'general'
    
    def _extract_entities(self, query: str) -> Dict[str, Any]:
        """ì—”í‹°í‹° ì¶”ì¶œ"""
        entities = {}
        
        for entity_type, patterns in self.entity_patterns.items():
            matches = []
            for pattern in patterns:
                found = re.findall(pattern, query, re.IGNORECASE)
                matches.extend(found)
            
            if matches:
                entities[entity_type] = list(set(matches))
        
        return entities
    
    def _enhance_with_ai(self, query: str, intent: str, entities: Dict[str, Any]) -> str:
        """AI ì„œë¹„ìŠ¤ë¥¼ í†µí•œ ì§ˆì˜ í–¥ìƒ"""
        
        # OpenAI API í™œìš© (ë¬´ë£Œ í‹°ì–´)
        if self.services['openai']['enabled']:
            try:
                return self._openai_enhancement(query, intent, entities)
            except Exception as e:
                logger.warning(f"OpenAI API ì˜¤ë¥˜: {e}")
        
        # Hugging Face í™œìš©
        if self.services['huggingface']['enabled']:
            try:
                return self._huggingface_enhancement(query, intent, entities)
            except Exception as e:
                logger.warning(f"Hugging Face API ì˜¤ë¥˜: {e}")
        
        # ê¸°ë³¸ í–¥ìƒ
        return self._basic_enhancement(query, intent, entities)
    
    def _openai_enhancement(self, query: str, intent: str, entities: Dict[str, Any]) -> str:
        """OpenAI APIë¥¼ í†µí•œ ì§ˆì˜ í–¥ìƒ"""
        try:
            headers = {
                'Authorization': f'Bearer {self.openai_api_key}',
                'Content-Type': 'application/json'
            }
            
            # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„±
            context = self._build_context(intent, entities)
            
            prompt = f"""
ë‹¤ìŒì€ ë¬´ì—­/ìˆ˜ì¶œì… ê´€ë ¨ ì§ˆë¬¸ì…ë‹ˆë‹¤. ë” êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ì§ˆë¬¸ìœ¼ë¡œ ê°œì„ í•´ì£¼ì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸: {query}
ì˜ë„: {intent}
ì¶”ì¶œëœ ì •ë³´: {json.dumps(entities, ensure_ascii=False)}

ì»¨í…ìŠ¤íŠ¸: {context}

ê°œì„ ëœ ì§ˆë¬¸:
"""
            
            data = {
                'model': 'gpt-3.5-turbo',
                'messages': [
                    {'role': 'system', 'content': 'ë‹¹ì‹ ì€ ë¬´ì—­/ìˆ˜ì¶œì… ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ì´ê³  ëª…í™•í•˜ê²Œ ê°œì„ í•´ì£¼ì„¸ìš”.'},
                    {'role': 'user', 'content': prompt}
                ],
                'max_tokens': 150,
                'temperature': 0.3
            }
            
            response = requests.post(
                f"{self.services['openai']['url']}chat/completions",
                headers=headers,
                json=data,
                timeout=10
            )
            
            if response.status_code == 200:
                result = response.json()
                return result['choices'][0]['message']['content'].strip()
            
        except Exception as e:
            logger.error(f"OpenAI API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        
        return query
    
    def _huggingface_enhancement(self, query: str, intent: str, entities: Dict[str, Any]) -> str:
        """Hugging Faceë¥¼ í†µí•œ ì§ˆì˜ í–¥ìƒ"""
        try:
            headers = {'Authorization': f'Bearer {self.huggingface_api_key}'}
            
            # í…ìŠ¤íŠ¸ ë¶„ë¥˜ë¥¼ í†µí•œ ì˜ë„ í™•ì¸
            data = {
                'inputs': query,
                'parameters': {
                    'candidate_labels': ['regulation', 'statistics', 'market_analysis', 'documentation', 'general']
                }
            }
            
            response = requests.post(
                f"{self.services['huggingface']['url']}{self.services['huggingface']['models']['text_classification']}",
                headers=headers,
                json=data,
                timeout=10
            )
            
            if response.status_code == 200:
                result = response.json()
                # ë¶„ë¥˜ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆì˜ ê°œì„ 
                return self._improve_query_based_on_classification(query, result)
            
        except Exception as e:
            logger.error(f"Hugging Face API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        
        return query
    
    def _basic_enhancement(self, query: str, intent: str, entities: Dict[str, Any]) -> str:
        """ê¸°ë³¸ ê·œì¹™ ê¸°ë°˜ ì§ˆì˜ í–¥ìƒ"""
        enhanced = query
        
        # êµ­ê°€ ì •ë³´ ì¶”ê°€
        if 'country' not in entities and intent in ['regulation', 'statistics']:
            if 'ì¤‘êµ­' in query or 'ì°¨ì´ë‚˜' in query:
                entities['country'] = ['ì¤‘êµ­']
            elif 'ë¯¸êµ­' in query or 'USA' in query:
                entities['country'] = ['ë¯¸êµ­']
        
        # ì œí’ˆ ì •ë³´ ì¶”ê°€
        if 'product' not in entities and 'ë¼ë©´' in query:
            entities['product'] = ['ë¼ë©´']
        
        # ì˜ë„ë³„ í‚¤ì›Œë“œ ì¶”ê°€
        if intent == 'regulation':
            if 'ê·œì œ' not in query and 'ì„œë¥˜' not in query:
                enhanced += " ê·œì œ ìš”êµ¬ì‚¬í•­"
        elif intent == 'statistics':
            if 'í†µê³„' not in query and 'ìˆ˜ì¹˜' not in query:
                enhanced += " í†µê³„ ë°ì´í„°"
        
        return enhanced
    
    def _build_context(self, intent: str, entities: Dict[str, Any]) -> str:
        """ì»¨í…ìŠ¤íŠ¸ êµ¬ì¶•"""
        context_parts = []
        
        if intent == 'regulation':
            context_parts.append("ê·œì œ ë° ì„œë¥˜ ìš”êµ¬ì‚¬í•­ ê´€ë ¨")
        elif intent == 'statistics':
            context_parts.append("ë¬´ì—­ í†µê³„ ë° ìˆ˜ì¹˜ ê´€ë ¨")
        elif intent == 'market_analysis':
            context_parts.append("ì‹œì¥ ë¶„ì„ ë° ë™í–¥ ê´€ë ¨")
        
        if 'country' in entities:
            countries = ', '.join(entities['country'])
            context_parts.append(f"ëŒ€ìƒ êµ­ê°€: {countries}")
        
        if 'product' in entities:
            products = ', '.join(entities['product'])
            context_parts.append(f"ì œí’ˆ: {products}")
        
        return ' | '.join(context_parts)
    
    def _calculate_confidence(self, query: str, intent: str, entities: Dict[str, Any]) -> float:
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        confidence = 0.5  # ê¸°ë³¸ê°’
        
        # ì—”í‹°í‹° ê°œìˆ˜ì— ë”°ë¥¸ ì ìˆ˜
        entity_count = len(entities)
        confidence += min(entity_count * 0.1, 0.3)
        
        # ì˜ë„ ëª…í™•ì„±
        if intent != 'general':
            confidence += 0.2
        
        # ì§ˆë¬¸ ê¸¸ì´ (ì ì ˆí•œ ê¸¸ì´ì¼ ë•Œ ë†’ì€ ì ìˆ˜)
        query_length = len(query)
        if 10 <= query_length <= 100:
            confidence += 0.1
        
        # íŠ¹ì • í‚¤ì›Œë“œ ì¡´ì¬
        specific_keywords = ['ë¼ë©´', 'ì¤‘êµ­', 'ë¯¸êµ­', 'ìˆ˜ì¶œ', 'ê·œì œ', 'í†µê³„']
        keyword_count = sum(1 for keyword in specific_keywords if keyword in query)
        confidence += min(keyword_count * 0.05, 0.2)
        
        return min(confidence, 1.0)
    
    def _determine_ai_service(self) -> str:
        """ì‚¬ìš©ëœ AI ì„œë¹„ìŠ¤ ê²°ì •"""
        if self.services['openai']['enabled']:
            return 'OpenAI GPT-3.5'
        elif self.services['huggingface']['enabled']:
            return 'Hugging Face'
        else:
            return 'Rule-based'
    
    def _fallback_processing(self, query: str) -> AIProcessedQuery:
        """í´ë°± ì²˜ë¦¬"""
        return AIProcessedQuery(
            original_query=query,
            processed_query=query,
            intent='general',
            entities={},
            confidence=0.5,
            ai_service_used='Rule-based (fallback)'
        )
    
    def generate_natural_response(self, query: str, data_results: Dict[str, Any]) -> str:
        """ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µ ìƒì„±"""
        try:
            # AI ì„œë¹„ìŠ¤ë¥¼ í†µí•œ ì‘ë‹µ ìƒì„±
            if self.services['openai']['enabled']:
                return self._generate_openai_response(query, data_results)
            elif self.services['huggingface']['enabled']:
                return self._generate_huggingface_response(query, data_results)
            else:
                return self._generate_improved_rule_based_response(query, data_results)
                
        except Exception as e:
            logger.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return self._generate_improved_rule_based_response(query, data_results)
    
    def _generate_openai_response(self, query: str, data_results: Dict[str, Any]) -> str:
        """ë¬´ë£Œ AI ì„œë¹„ìŠ¤ë¥¼ í†µí•œ ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µ ìƒì„±"""
        try:
            # ë°ì´í„° ìš”ì•½
            data_summary = self._summarize_data_for_ai(data_results)
            
            # ë¬´ë£Œ AI ì„œë¹„ìŠ¤ í™œìš© (Hugging Face ë˜ëŠ” ë¡œì»¬ ëª¨ë¸)
            if self.huggingface_api_key:
                return self._generate_huggingface_response(query, data_summary)
            else:
                return self._generate_improved_rule_based_response(query, data_results)
            
        except Exception as e:
            logger.error(f"AI ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
            return self._generate_improved_rule_based_response(query, data_results)
    
    def _generate_huggingface_response(self, query: str, data_summary: str) -> str:
        """Hugging Faceë¥¼ í†µí•œ ì‘ë‹µ ìƒì„±"""
        try:
            headers = {'Authorization': f'Bearer {self.huggingface_api_key}'}
            
            # í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ ì‚¬ìš©
            prompt = f"ì§ˆë¬¸: {query}\në°ì´í„°: {data_summary}\në‹µë³€:"
            
            data = {
                'inputs': prompt,
                'parameters': {
                    'max_length': 300,
                    'temperature': 0.7,
                    'do_sample': True
                }
            }
            
            response = requests.post(
                f"{self.services['huggingface']['url']}{self.services['huggingface']['models']['text_generation']}",
                headers=headers,
                json=data,
                timeout=15
            )
            
            if response.status_code == 200:
                result = response.json()
                return result[0]['generated_text'].split('ë‹µë³€:')[-1].strip()
            
        except Exception as e:
            logger.error(f"Hugging Face ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
        
        return self._generate_improved_rule_based_response(query, {})
    
    def _generate_improved_rule_based_response(self, query: str, data_results: Dict[str, Any]) -> str:
        """ê°œì„ ëœ ê·œì¹™ ê¸°ë°˜ ì‘ë‹µ ìƒì„± (ë” ìì—°ìŠ¤ëŸ½ê²Œ)"""
        response_parts = []
        
        # ì§ˆë¬¸ ì˜ë„ì— ë”°ë¥¸ ìì—°ìŠ¤ëŸ¬ìš´ ì‹œì‘
        if 'ê·œì œ' in query or 'ì„œë¥˜' in query or 'ì¸ì¦' in query:
            response_parts.append("ğŸ” **ê·œì œ ì •ë³´ ì•ˆë‚´**")
        elif 'í†µê³„' in query or 'ìˆ˜ì¹˜' in query or 'ë°ì´í„°' in query:
            response_parts.append("ğŸ“Š **ë¬´ì—­ í†µê³„ í˜„í™©**")
        elif 'ì‹œì¥' in query or 'ë™í–¥' in query or 'ì „ë§' in query:
            response_parts.append("ğŸ“ˆ **ì‹œì¥ ë¶„ì„ ê²°ê³¼**")
        else:
            response_parts.append("ğŸ’¡ **ìˆ˜ì¶œ ì •ë³´ ì•ˆë‚´**")
        
        # ê·œì œ ì •ë³´ (ì¤‘ë³µ ì œê±° ë° ìì—°ìŠ¤ëŸ½ê²Œ)
        if data_results.get('regulations'):
            regulations = data_results['regulations']
            # ì¤‘ë³µ ì œê±°
            unique_regs = []
            seen = set()
            for reg in regulations:
                key = f"{reg[1]}_{reg[2]}"
                if key not in seen:
                    unique_regs.append(reg)
                    seen.add(key)
            
            for reg in unique_regs[:2]:  # ìµœëŒ€ 2ê°œë§Œ
                country = reg[1]
                product = reg[2]
                title = reg[3]
                description = reg[4]
                requirements = reg[5]
                
                response_parts.append(f"\n**{country} {product} ìˆ˜ì¶œ ê·œì œ**")
                if description and description != title:
                    response_parts.append(f"â€¢ {description}")
                
                if requirements and requirements != description:
                    # ìš”êµ¬ì‚¬í•­ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ í‘œì‹œ
                    if 'ì„œë¥˜' in requirements or 'ì¸ì¦' in requirements:
                        response_parts.append(f"â€¢ **í•„ìš” ì„œë¥˜**: {requirements}")
                    else:
                        response_parts.append(f"â€¢ **ì£¼ìš” ìš”êµ¬ì‚¬í•­**: {requirements}")
                
                response_parts.append(f"â€¢ **ì¶œì²˜**: {reg[7]}")
        
        # ë¬´ì—­ í†µê³„
        if data_results.get('trade_statistics'):
            response_parts.append("\nğŸ“Š **ë¬´ì—­ í†µê³„ í˜„í™©**")
            for stat in data_results['trade_statistics'][:2]:
                country = stat[0]
                product = stat[2]
                period = stat[3]
                export_amount = stat[5]
                import_amount = stat[6]
                growth_rate = stat[8]
                
                response_parts.append(f"\n**{country} {product} ({period})**")
                response_parts.append(f"â€¢ ìˆ˜ì¶œì•¡: {export_amount:,.0f}ë§Œì›")
                response_parts.append(f"â€¢ ìˆ˜ì…ì•¡: {import_amount:,.0f}ë§Œì›")
                if growth_rate:
                    response_parts.append(f"â€¢ ì„±ì¥ë¥ : {growth_rate:+.1f}%")
                response_parts.append(f"â€¢ ì¶œì²˜: {stat[10]}")
        
        # ì‹œì¥ ë¶„ì„
        if data_results.get('market_analysis'):
            response_parts.append("\nğŸ“ˆ **ì‹œì¥ ë¶„ì„ ê²°ê³¼**")
            for analysis in data_results['market_analysis'][:1]:
                title = analysis[4]
                content = analysis[5]
                response_parts.append(f"\n**{title}**")
                response_parts.append(f"â€¢ {content[:150]}...")
                response_parts.append(f"â€¢ ì¶œì²˜: {analysis[9]}")
        
        # ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°
        if not any([data_results.get('regulations'), data_results.get('trade_statistics'), data_results.get('market_analysis')]):
            response_parts.append("\nğŸ˜” **ì£„ì†¡í•©ë‹ˆë‹¤**")
            response_parts.append("í˜„ì¬ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            response_parts.append("ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ ë³´ì‹œê±°ë‚˜, ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.")
        
        return '\n'.join(response_parts)
    
    def _summarize_data_for_ai(self, data_results: Dict[str, Any]) -> str:
        """AIìš© ë°ì´í„° ìš”ì•½"""
        summary_parts = []
        
        if data_results.get('regulations'):
            summary_parts.append("ğŸ“‹ ê·œì œ ì •ë³´:")
            for reg in data_results['regulations'][:2]:
                summary_parts.append(f"- {reg[1]} {reg[2]}: {reg[4]}")
        
        if data_results.get('trade_statistics'):
            summary_parts.append("ğŸ“Š ë¬´ì—­ í†µê³„:")
            for stat in data_results['trade_statistics'][:2]:
                summary_parts.append(f"- {stat[0]} {stat[2]}: ìˆ˜ì¶œ {stat[5]:,.0f}ë§Œì›, ìˆ˜ì… {stat[6]:,.0f}ë§Œì›")
        
        if data_results.get('market_analysis'):
            summary_parts.append("ğŸ“ˆ ì‹œì¥ ë¶„ì„:")
            for analysis in data_results['market_analysis'][:1]:
                summary_parts.append(f"- {analysis[4]}: {analysis[5][:100]}...")
        
        return '\n'.join(summary_parts) 